#! /usr/bin/env python3

# ======================================================================================================================
# Juneberry - General Release
#
# Copyright 2021 Carnegie Mellon University.
#
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS"
# BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER
# INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED
# FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM
# FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
#
# Released under a BSD (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
#
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see
# Copyright notice for non-US Government use and distribution.
#
# This Software includes and/or makes use of Third-Party Software subject to its own license.
#
# DM21-0884
#
# ======================================================================================================================

# Read the experiment config and splat it out to a graph.

import argparse
import logging
from pathlib import Path
import sys

from juneberry.config.model import ModelConfig
from juneberry.config.experiment import ExperimentConfig, Filter, Report, ReportType
from juneberry.config.rule_list import RuleListBuilder
import juneberry.filesystem as jbfs
from juneberry.filesystem import ModelManager

logger = logging.getLogger(__name__)

JB_TRAIN_COMMAND = 'jb_train'
JB_EVALUATE_COMMAND = 'jb_evaluate'
JB_REPORT_COMMAND = 'jb_report'


class RuleMaker:
    def __init__(self, experiment_name):
        self.experiment_name = experiment_name
        self.tag_map = {}
        self.experiment_manager = jbfs.ExperimentManager(self.experiment_name)
        self.experiment_config = ExperimentConfig.load(self.experiment_manager.get_experiment_config())
        self.builder = RuleListBuilder("")

        self.filter_map = {}
        if self.experiment_config.filters:
            self.filter_map = {i.tag: i for i in self.experiment_config.filters}

    def save(self, file_path):

        self.add_training()
        self.add_eval()
        self.add_reports()

        self.builder.rules_list.save(file_path)

    def add_training(self):
        for model in self.experiment_config.models:
            model_mgr = jbfs.ModelManager(model.name)
            model_config = ModelConfig.load(model_mgr.get_model_config())

            # Training rules
            workflow = self.builder.get_workflow("main")
            inputs = [model_mgr.get_model_config(), model_config.training_dataset_config_path]
            outputs = model_mgr.get_training_output_list()
            command = [JB_TRAIN_COMMAND, model_mgr.model_name]

            # Add the "--onnx" option to jb_train if an ONNX model was requested.
            if model.onnx:
                command.append("--onnx")

            doc = " ".join([str(x) for x in command])
            clean_extras = model_mgr.get_training_clean_extras_list()
            requirements = []

            # Add a temporary property for now
            model['train_rule_id'] = workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

            # Dry run rules
            workflow = self.builder.get_workflow("dryrun")
            inputs = [model_mgr.get_model_config(), model_config.training_dataset_config_path]
            outputs = model_mgr.get_dry_run_output_list()
            command = [JB_TRAIN_COMMAND, model_mgr.model_name, '--dryrun']
            doc = " ".join([str(x) for x in command])
            clean_extras = model_mgr.get_dry_run_clean_extras_list()
            requirements = []

            # Add a temporary property for now
            model['dryrun_rule_id'] = workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

            if model.filters is not None:
                self.add_filters(model.filters, model_mgr, model_config.training_dataset_config_path,
                                 model['train_rule_id'])

    def add_eval(self):
        self.tag_map = {}

        for model in self.experiment_config.models:
            model_mgr = jbfs.ModelManager(model.name)

            workflow = self.builder.get_workflow("main")
            """
                "tag": <internal tag that reports references>,
                "datasetPath": <A path to a data set config file>,
                "classify": <Integer that controls how many of the top predicted classes get recorded>
            """

            # Predictions
            if model.tests:
                for test in model.tests:
                    inputs = [model_mgr.get_model_path(), test.dataset_path]
                    outputs = model_mgr.get_evaluation_output_list(test.dataset_path)
                    command = [JB_EVALUATE_COMMAND, model_mgr.model_name, test.dataset_path]
                    if test.classify:
                        command.extend(["--topK", test.classify])
                    if test.use_train_split:
                        command.append('--useTrainSplit')
                    if test.use_val_split:
                        command.append('--useValSplit')
                    doc = " ".join([str(x) for x in command])
                    clean_extras = model_mgr.get_predictions_clean_extras_list()
                    requirements = [model['train_rule_id']]

                    cmd_id = workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

                    # Save the rule id and file for the report
                    eval_dir_mgr = model_mgr.get_eval_dir_mgr(test.dataset_path)
                    self.tag_map[test.tag] = {
                        'dataset': test.dataset_path,
                        'filename': eval_dir_mgr.get_predictions_path(),
                        'id': cmd_id,
                        'model': model.name,
                        'metrics': eval_dir_mgr.get_metrics_path()
                    }

                    if test.filters:
                        self.add_filters(test.filters, model_mgr, test.dataset_path, cmd_id)

    def add_reports(self):

        inputs = []
        requirements = []
        outputs = []

        workflow_components = (inputs, requirements, outputs)

        for report in self.experiment_config.reports:

            if report.fqcn == ReportType.PLOT_ROC:
                report, workflow_components = self.add_plot_roc(report, workflow_components)
            elif report.fqcn == ReportType.PLOT_PR:
                report, workflow_components = self.add_plot_pr(report, workflow_components)
            elif report.fqcn == ReportType.SUMMARY:
                report, workflow_components = self.add_summary(report, workflow_components)

        self.experiment_config.save(self.experiment_manager.get_experiment_config())
        self.add_jb_report(workflow_components)

    def add_plot_pr(self, report: Report, workflow_components: tuple):

        inputs, requirements, outputs = workflow_components

        # If an output_dir was not provided in the kwargs, use the experiment directory.
        if "output_dir" not in report.kwargs:
            report.kwargs.output_dir = str(self.experiment_manager.experiment_dir_path)
        else:
            output_dir_path = Path(report.kwargs.output_dir)
            if output_dir_path not in self.experiment_manager.experiment_dir_path.parents:
                report.kwargs.output_dir = str(self.experiment_manager.experiment_dir_path / output_dir_path)

        curve_sources = report.kwargs.curve_sources if "curve_sources" in report.kwargs else {}
        # Add all tests to the command.
        for test in report.tests:

            curve_sources[self.tag_map[test.tag]['model']] = self.tag_map[test.tag]['dataset']

            # The presence of the metrics file indicates an evaluation has occurred for this
            # model / dataset combination.
            metrics_file = str(self.tag_map[test.tag]['metrics'])
            inputs.append(metrics_file)

            requirements.append(self.tag_map[test.tag]['id'])

        output_files = ["pr_curve.png", "pc_curve.png", "rc_curve.png", "eval_metrics.csv"]
        for output_file in output_files:
            output_str = str(Path(report.kwargs.output_dir, output_file))
            if output_str not in outputs:
                outputs.append(output_str)

        report.kwargs.curve_sources = curve_sources

        return report, (inputs, requirements, outputs)

    def add_plot_roc(self, report: Report, workflow_components: tuple):

        inputs, requirements, outputs = workflow_components

        if "output_str" not in report.kwargs:
            report.kwargs.output_str = str(self.experiment_manager.experiment_dir_path)
        else:
            report.kwargs.output_str = str(self.experiment_manager.get_experiment_file(report.output_name))
        outputs.append(report.kwargs.output_str)

        curve_sources = report.kwargs.curve_sources if "curve_sources" in report.kwargs else {}
        # Accumulate all the tests (predictions) into one report
        for test in report.tests:
            predictions_file = str(self.tag_map[test.tag]['filename'])
            classes = test.classes if test.classes else report.classes
            curve_sources[predictions_file] = classes
            inputs.append(predictions_file)

            requirements.append(self.tag_map[test.tag]['id'])

        return report, (inputs, requirements, outputs)

    def add_summary(self, report: Report, workflow_components: tuple):

        inputs, requirements, outputs = workflow_components

        if report.kwargs.md_str:
            md_path = Path(report.kwargs.md_str)

            if len(md_path.parents) == 1:
                report.kwargs.md_str = self.experiment_manager.experiment_dir_path / md_path

        else:
            report.kwargs.md_str = str(self.experiment_manager.experiment_dir_path / "summary.md")

        outputs.append(report.kwargs.md_str)

        if report.kwargs.csv_str:
            csv_path = Path(report.kwargs.csv_str)

            if len(csv_path.parents) == 1:
                report.kwargs.csv_str = self.experiment_manager.experiment_dir_path / csv_path

            outputs.append(report.kwargs.csv_str)

        metrics_files = report.kwargs.metrics_files if 'metrics_files' in report.kwargs else []
        for entry in self.tag_map.values():
            inputs.append(entry['metrics'])
            if entry['metrics'] not in metrics_files:
                metrics_files.append(entry['metrics'])
            requirements.append(entry['id'])

        report.kwargs.metrics_files = metrics_files

        plot_files = report.kwargs.plot_files if 'plot_files' in report.kwargs else []
        # We need all the output file names. Depending on the report type, different plots may
        # get added to the summary file.

        for r in self.experiment_config.reports:
            # Add ROC plots to the summary file, if necessary.
            if r.fqcn == ReportType.PLOT_ROC:
                # TODO: Need to add any ROC curve files to the plot_files
                continue

            # Add PR plots to the summary file, if necessary.
            elif r.fqcn == ReportType.PLOT_PR:
                # TODO: These names should come from the plotting curve code once we refactor jb_plot_pr
                plot_filenames = ["pr_curve.png", "pc_curve.png", "rc_curve.png"]

                for file_name in plot_filenames:
                    plot_file = str(Path(r.kwargs.output_dir) / file_name)
                    if plot_file not in plot_files:
                        plot_files.append(plot_file)

            else:
                continue

        report.kwargs.plot_files = plot_files

        return report, (inputs, requirements, outputs)

    def add_jb_report(self, workflow_components: tuple):
        inputs, requirements, outputs = workflow_components

        inputs = list(set(inputs))
        outputs = list(set(outputs))

        command = [JB_REPORT_COMMAND, '-e', self.experiment_name]
        doc = " ".join([str(x) for x in command])

        clean_extras = []

        workflow = self.builder.get_workflow("main")
        workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

    # ==============================================================================================

    #  _____ _ _ _
    # |  ___(_) | |_ ___ _ __ ___
    # | |_  | | | __/ _ \ '__/ __|
    # |  _| | | | ||  __/ |  \__ \
    # |_|   |_|_|\__\___|_|  |___/

    def add_filters(self, filters, model_mgr: ModelManager, dataset_path, cmd_id: int):
        for tag in filters:
            # Grab the filter from our map and fill in the entries.
            my_filter: Filter = self.filter_map.get(tag, None)
            if my_filter is None:
                logger.error(f"Can't find filter {tag} in filter list. Exiting.")
                sys.exit(-1)

            # Convert the replacement fields.
            command = [self.format_arg(i, model_mgr, dataset_path, tag) for i in my_filter.cmd]
            inputs = []
            if my_filter.inputs is not None:
                inputs = [self.format_arg(i, model_mgr, dataset_path, tag) for i in my_filter.inputs]
            outputs = []
            if my_filter.outputs is not None:
                outputs = [self.format_arg(i, model_mgr, dataset_path, tag) for i in my_filter.outputs]
            requirements = [cmd_id]

            # We are guessing here.
            doc = f"Filter - '{model_mgr.model_name}' with '{command[0]}'"
            clean_extras = []

            workflow = self.builder.get_workflow("main")
            workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

    @staticmethod
    def format_arg(arg: str, model_mgr: ModelManager, dataset_path: str, filter_tag: str):
        # This is basically just a glorified format string
        replace_map = {
            'model_name': Path(model_mgr.model_name),
            'train_output': Path(model_mgr.get_training_out_file())
        }
        if dataset_path is not None:
            replace_map['dataset_path'] = Path(dataset_path)
            replace_map['eval_predictions'] = model_mgr.get_eval_dir_mgr(dataset_path).get_predictions_path()

        # TODO: Add support for workspace and dataroot

        try:
            return arg.format(**replace_map)
        except KeyError:
            logger.error(f"Failed to convert the arg '{arg}' in filter '{filter_tag}' during replacement. Exiting.")
            sys.exit(-1)


# ==================================================================================================

def main():
    parser = argparse.ArgumentParser(description="Generates a rules list in the experiment directory from an "
                                                 "experiment config.")
    parser.add_argument('experimentName', help='Name of the experiment in the experiments directory.')

    args = parser.parse_args()

    maker = RuleMaker(args.experimentName)
    experiment_manager = jbfs.ExperimentManager(args.experimentName)
    maker.save(experiment_manager.get_experiment_rules())

    logger.info(f"jb_experiment_to_rules is done.")


if __name__ == "__main__":
    main()
