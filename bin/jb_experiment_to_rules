#! /usr/bin/env python3

# ======================================================================================================================
# Juneberry - General Release
#
# Copyright 2021 Carnegie Mellon University.
#
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS"
# BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER
# INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED
# FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM
# FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
#
# Released under a BSD (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
#
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see
# Copyright notice for non-US Government use and distribution.
#
# This Software includes and/or makes use of Third-Party Software subject to its own license.
#
# DM21-0884
#
# ======================================================================================================================

# Read the experiment config and splat it out to a graph.

import argparse
import json
import logging
from pathlib import Path
from prodict import List, Prodict
import sys

from juneberry.config.experiment import ExperimentConfig, Filter, Report, ReportType
from juneberry.config.model import ModelConfig
from juneberry.config.rule_list import RuleListBuilder
from juneberry.filesystem import ExperimentManager, ModelManager

logger = logging.getLogger(__name__)

JB_TRAIN_COMMAND = 'jb_train'
JB_EVALUATE_COMMAND = 'jb_evaluate'
JB_REPORT_COMMAND = 'jb_report'


class RuleMaker:
    def __init__(self, experiment_name):
        self.experiment_name = experiment_name
        self.tag_map = {}
        self.experiment_manager = ExperimentManager(self.experiment_name)
        self.experiment_config = ExperimentConfig.load(self.experiment_manager.get_experiment_config())
        self.builder = RuleListBuilder("")

        self.filter_map = {}
        if self.experiment_config.filters:
            self.filter_map = {i.tag: i for i in self.experiment_config.filters}

    def save(self, file_path):

        self.add_training()
        self.add_eval()
        self.add_reports()

        self.builder.rules_list.save(file_path)

    def add_training(self):
        for model in self.experiment_config.models:
            model_mgr = ModelManager(model.name)
            model_config = ModelConfig.load(model_mgr.get_model_config())

            # Training rules
            workflow = self.builder.get_workflow("main")
            inputs = [model_mgr.get_model_config(), model_config.training_dataset_config_path]
            outputs = model_mgr.get_training_output_list()
            command = [JB_TRAIN_COMMAND, model_mgr.model_name]

            # Add the "--onnx" option to jb_train if an ONNX model was requested.
            if model.onnx:
                command.append("--onnx")

            doc = " ".join([str(x) for x in command])
            clean_extras = model_mgr.get_training_clean_extras_list()
            requirements = []

            # Add a temporary property for now
            model['train_rule_id'] = workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

            # Dry run rules
            workflow = self.builder.get_workflow("dryrun")
            inputs = [model_mgr.get_model_config(), model_config.training_dataset_config_path]
            outputs = model_mgr.get_dry_run_output_list()
            command = [JB_TRAIN_COMMAND, model_mgr.model_name, '--dryrun']
            doc = " ".join([str(x) for x in command])
            clean_extras = model_mgr.get_dry_run_clean_extras_list()
            requirements = []

            # Add a temporary property for now
            model['dryrun_rule_id'] = workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

            if model.filters is not None:
                self.add_filters(model.filters, model_mgr, model_config.training_dataset_config_path,
                                 model['train_rule_id'])

    def add_eval(self):
        self.tag_map = {}

        for model in self.experiment_config.models:
            model_mgr = ModelManager(model.name)

            workflow = self.builder.get_workflow("main")
            """
                "tag": <internal tag that reports references>,
                "datasetPath": <A path to a data set config file>,
                "classify": <Integer that controls how many of the top predicted classes get recorded>
            """

            # Predictions
            if model.tests:
                for test in model.tests:
                    inputs = [model_mgr.get_model_path(), test.dataset_path]
                    outputs = model_mgr.get_evaluation_output_list(test.dataset_path)
                    command = [JB_EVALUATE_COMMAND, model_mgr.model_name, test.dataset_path]
                    if test.classify:
                        command.extend(["--topK", test.classify])
                    if test.use_train_split:
                        command.append('--useTrainSplit')
                    if test.use_val_split:
                        command.append('--useValSplit')
                    doc = " ".join([str(x) for x in command])
                    clean_extras = model_mgr.get_predictions_clean_extras_list()
                    requirements = [model['train_rule_id']]

                    cmd_id = workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

                    # Save the rule id and file for the report
                    eval_dir_mgr = model_mgr.get_eval_dir_mgr(test.dataset_path)
                    self.tag_map[test.tag] = {
                        'dataset': test.dataset_path,
                        'filename': eval_dir_mgr.get_predictions_path(),
                        'id': cmd_id,
                        'model': model.name,
                        'metrics': eval_dir_mgr.get_metrics_path()
                    }

                    if test.filters:
                        self.add_filters(test.filters, model_mgr, test.dataset_path, cmd_id)

    def add_reports(self) -> None:
        # Add Reports when the experiment config contains a "reports" stanza.
        if self.experiment_config.reports:

            # The goal is to create an individual report.json file for every report
            # listed in the reports stanza. All of these reports will be stored in
            # the 'report_json_files' sub-directory inside the experiment directory.
            # Detect if that sub-directory exists, and create it when it does not.
            if not self.experiment_manager.experiment_reports_dir_path.exists():
                self.experiment_manager.experiment_reports_dir_path.mkdir()

            # Now loop through every report in the "reports" stanza. The 'idx' is
            # used to create a unique name for each report.json file.
            for idx, report in enumerate(self.experiment_config.reports):

                # Add a properly formatted 'jb_report' rule to generate ROC curves
                # when the report type matches the PLOT_ROC class.
                if report.fqcn == ReportType.PLOT_ROC:
                    self.add_plot_roc(report, idx)

                # Add a properly formatted 'jb_report' rule to generate PR curves
                # when the report type matches the PLOT_PR class.
                elif report.fqcn == ReportType.PLOT_PR:
                    self.add_plot_pr(report, idx)

                # Add a properly formatted 'jb_report' rule to generate a summary
                # report when the report type matches the SUMMARY class.
                elif report.fqcn == ReportType.SUMMARY:
                    self.add_summary(report, idx)

                # For all other report FQCNs, follow the process for adding a
                # generic report rule.
                else:
                    self.add_generic_report(report, idx)

    def add_plot_pr(self, report: Report, idx: int) -> None:
        """
        This method is responsible for adding a rule to the workflow that will use the 'jb_report'
        command to produce PR curves according to the conditions described in the report stanza.
        :param report: One Report stanza from the list of reports described in the "reports" stanza
        of the experiment config.
        :param idx: An integer describing the position of the report parameter within the list of
        reports in the experiment config's "reports" stanza. Used to produce a unique filename.
        :return: Nothing.
        """

        # Set up some lists to track some information for pydoit.
        inputs = []
        requirements = []
        outputs = []

        # Use the idx to construct a unique filename for the individual JSON file that will contain
        # ONLY the information for this report.
        report_json_name = f"report_{idx}.json"
        report_json_path = self.experiment_manager.experiment_reports_dir_path / report_json_name

        # The individual JSON file for this report is a product of this process, so add it to the
        # list of outputs.
        outputs.append(str(report_json_path))

        # Initialize the contents of the JSON file to match the desired report.
        report_json_content = report

        # If an output_dir was not provided in the report's kwargs, use the experiment directory.
        if "output_dir" not in report.kwargs:
            logger.warning(f"An output_dir was not found in the kwargs of report {idx} in the experiment "
                           f"config's reports. Setting the output_dir to the experiment dir.")
            report_json_content.kwargs.output_dir = str(self.experiment_manager.experiment_dir_path)

        # Otherwise attempt to use the "output_dir" provided in the config.
        else:
            # Convert the output_dir arg from a string to a Path.
            output_dir_path = Path(report.kwargs.output_dir)

            # No matter what output_dir the user provided via the config, we want to make sure their desired
            # directory is inside the current experiment directory. So, resolve both Paths and then check if
            # the experiment directory matches any of the parent directories for the user's desired output
            # directory. If the desired output_dir doesn't appear to be inside the experiment dir, then adjust
            # the output dir to be a sub-dir inside the experiment directory.
            if self.experiment_manager.experiment_dir_path.resolve() not in output_dir_path.resolve().parents:
                new_output_dir_str = str(self.experiment_manager.experiment_dir_path / output_dir_path)
                report_json_content.kwargs.output_dir = new_output_dir_str
                logger.warning(f"The desired output_dir in report {idx} does not appear to be inside the "
                               f"experiment directory. Adjusting the value for the output_dir to "
                               f"{new_output_dir_str}")

        # The output_dir for this report is a product of this process, so add it to the list of outputs.
        outputs.append(report_json_content.kwargs.output_dir)

        # Retrieve the desired curve sources for this Report, if any were provided.
        curve_sources = report.kwargs.curve_sources if "curve_sources" in report.kwargs else {}

        # According to the experiment config, a report must have a tests section in order to make sense.
        # So, loop through those tests and add the appropriate curve sources.
        for test in report.tests:
            # Add the curve source to the dictionary, where the key is the name of the model, and the value
            # is the name of the dataset that the model evaluated. This model:dataset combination indicates
            # which evaluation data to use when generating these PR curves.
            curve_sources[self.tag_map[test.tag]['model']] = self.tag_map[test.tag]['dataset']

            # The presence of the metrics file indicates an evaluation has occurred for this
            # model / dataset combination. Since the PR curves can't be generated without the evaluation
            # data, then this file should be treated as an input for this task in the workflow.
            metrics_file = str(self.tag_map[test.tag]['metrics'])
            inputs.append(metrics_file)

            requirements.append(self.tag_map[test.tag]['id'])

        # The result of this report will produce 3 PNG files with different figures. Those files must
        # be tracked as outputs of this rule.
        output_files = ["pr_curve.png", "pc_curve.png", "rc_curve.png"]
        for output_file in output_files:
            # Construct the Path in the output_dir for this particular output file and then convert
            # it to a string so it can be written to the rules file. Add the string to the list of outputs.
            output_str = str(Path(report_json_content.kwargs.output_dir, output_file))
            outputs.append(output_str)

        # If any changes were made to the curve_sources, the content for the JSON file should reflect
        # those changes.
        report_json_content.kwargs.curve_sources = curve_sources

        # Create the individual JSON file for this report and add the rule to the workflow.
        self._finalize_report(report_json_content, report_json_path, inputs, requirements, outputs)

    def add_plot_roc(self, report: Report, idx: int) -> None:
        """
        This method is responsible for adding a rule to the workflow that will use the 'jb_report'
        command to produce ROC curves according to the conditions described in the report stanza.
        :param report: One Report stanza from the list of reports described in the "reports" stanza
        of the experiment config.
        :param idx: An integer describing the position of the report parameter within the list of
        reports in the experiment config's "reports" stanza. Used to produce a unique filename.
        :return: Nothing.
        """

        # Set up some lists to track some information for pydoit.
        inputs = []
        requirements = []
        outputs = []

        # Use the idx to construct a unique filename for the individual JSON file that will contain
        # ONLY the information for this report.
        report_json_name = f"report_{idx}.json"
        report_json_path = self.experiment_manager.experiment_reports_dir_path / report_json_name
        report_json_content = report

        # If an output_filename was not provided in the report's kwargs, use the experiment directory and
        # a default filename.
        if "output_filename" not in report.kwargs:
            logger.warning(f"An output_filename was not found in the kwargs of report {idx} in the experiment "
                           f"config's reports. Setting the output_filename to the default.")
            default_output_filename = self.experiment_manager.experiment_dir_path / "ROC_curves.png"
            logger.warning(f"  Setting 'output_filename' to {default_output_filename}")
            report_json_content.kwargs.output_filename = str(default_output_filename)

        # Otherwise attempt to use the "output_filename" provided in the config.
        else:
            # Convert the output_filename from a string to a Path.
            output_filename_path = Path(report.kwargs.output_filename)

            # Check if the desired output_filename is inside the experiment directory. If it is not,
            # then place the experiment directory at the root of the desired output_filename and use
            # that new Path as the output_filename.
            if self.experiment_manager.experiment_dir_path.resolve() not in output_filename_path.resolve().parents:
                new_output_filename = self.experiment_manager.experiment_dir_path / output_filename_path
                report_json_content.kwargs.output_filename = str(new_output_filename)

        # The output_filename for this report is a product of this process, so add it to the list of outputs.
        outputs.append(report_json_content.kwargs.output_filename)

        # Retrieve the desired curve sources for this Report, if any were provided.
        curve_sources = report.kwargs.curve_sources if "curve_sources" in report.kwargs else {}

        # Accumulate all the tests (predictions) into one report.
        for test in report.tests:
            predictions_file = str(self.tag_map[test.tag]['filename'])

            # The desired classes for the ROC curve can come from two places: inside the
            # test itself or a 'classes' field in the report stanza. The field inside the
            # test itself has priority.
            if test.classes:
                classes = test.classes
            else:
                if report.classes:
                    classes = report.classes
                # If no classes were specified, then just plot all known classes.
                else:
                    classes = "all"

            # Add the 'prediction_file:desired_classes' combo to the curve sources. Since the
            # predictions file needs to exist in order to produce this ROC curve, count it as an input.
            curve_sources[predictions_file] = classes
            inputs.append(predictions_file)

            requirements.append(self.tag_map[test.tag]['id'])

        # If any changes were made to the curve_sources, the content for the JSON file should reflect
        # those changes.
        report_json_content.kwargs.curve_sources = curve_sources

        # Create the individual JSON file for this report and add the rule to the workflow.
        self._finalize_report(report_json_content, report_json_path, inputs, requirements, outputs)

    def add_summary(self, report: Report, idx: int) -> None:
        """
        This method is responsible for adding a rule to the workflow that will use the 'jb_report'
        command to produce a Summary Report according to the conditions described in the report stanza.
        :param report: One Report stanza from the list of reports described in the "reports" stanza
        of the experiment config.
        :param idx: An integer describing the position of the report parameter within the list of
        reports in the experiment config's "reports" stanza. Used to produce a unique filename.
        :return: Nothing.
        """
        # Set up some lists to track some information for pydoit.
        inputs = []
        requirements = []
        outputs = []

        # Use the idx to construct a unique filename for the individual JSON file that will contain
        # ONLY the information for this report.
        report_json_name = f"report_{idx}.json"
        report_json_path = self.experiment_manager.experiment_reports_dir_path / report_json_name

        # The individual JSON file for this report is a product of this process, so add it to the
        # list of outputs.
        outputs.append(str(report_json_path))

        # Initialize the contents of the JSON file to match the desired report.
        report_json_content = report

        # If an md_filename was not provided in the report's kwargs, use the experiment directory and
        # a default filename.
        if "md_filename" not in report.kwargs:
            logger.warning(f"An md_filename was not found in the kwargs of report {idx} in the experiment "
                           f"config's reports. Setting the output_filename to the default.")
            default_md_filename = self.experiment_manager.experiment_dir_path / "summary.md"
            logger.warning(f"  Setting 'output_filename' to {default_md_filename}")
            report_json_content.kwargs.output_filename = str(default_md_filename)

        # Otherwise attempt to use the "md_filename" provided in the config.
        else:
            # Convert the md_filename from a string to a Path.
            md_filename_path = Path(report.kwargs.md_filename)

            # Check if the desired md_filename is inside the experiment directory. If it is not,
            # then place the experiment directory at the root of the desired md_filename and use
            # that new Path as the md_filename.
            if self.experiment_manager.experiment_dir_path.resolve() not in md_filename_path.resolve().parents:
                new_md_filename = self.experiment_manager.experiment_dir_path / md_filename_path
                logger.warning(f"The desired md_filename did not appear to be in the experiment directory. "
                               f"Adjusting the value for md_filename to {new_md_filename}")
                report_json_content.kwargs.md_filename = str(new_md_filename)

        # The md_filename for this report is a product of this process, so add it to the list of outputs.
        outputs.append(report_json_content.kwargs.md_filename)

        # The CSV file is optional, so only adjust the filename (potentially) if a csv_filename was provided.
        if report.kwargs.csv_filename:
            # Convert the csv_filename from a string to a Path.
            csv_path = Path(report.kwargs.csv_filename)

            # Check if the desired csv_filename is inside the experiment directory. If it is not,
            # then place the experiment directory at the root of the desired csv_filename and use
            # that new Path as the csv_filename.
            if self.experiment_manager.experiment_dir_path.resolve() not in csv_path.resolve().parents:
                new_csv_filename_path = self.experiment_manager.experiment_dir_path / csv_path
                logger.warning(f"The desired csv_filename did not appear to be in the experiment directory. "
                               f"Adjusting the value for csv_filename to {new_csv_filename_path}")
                report_json_content.kwargs.csv_filename = str(new_csv_filename_path)

            # The csv_filename for this report is a product of this process, so add it to the list of outputs.
            outputs.append(report_json_content.kwargs.csv_filename)

        # Retrieve the desired metrics files to include in this Report, if any were provided.
        metrics_files = report.kwargs.metrics_files if 'metrics_files' in report.kwargs else []

        # Loop through the test tags, add any metrics files to the list of inputs, and if the metrics file
        # isn't represented in the summary, add it there as well.
        for entry in self.tag_map.values():
            inputs.append(entry['metrics'])
            if entry['metrics'] not in metrics_files:
                metrics_files.append(entry['metrics'])
            requirements.append(entry['id'])

        # If any changes were made to the metrics_files, the content for the JSON file should reflect
        # those changes.
        report_json_content.kwargs.metrics_files = metrics_files

        # Follow a similar process for the plot files to include in the summary. Start by retrieving any
        # plot files specified in the report stanza.
        plot_files = report.kwargs.plot_files if 'plot_files' in report.kwargs else []

        # Next, look through all of the Reports listed in the experiment config and add any plot files created
        # there to the summary. Depending on the report type, different plots may get added to the summary file.
        for index, report in enumerate(self.experiment_config.reports):

            # When the index values match, that means the loop is looking at the report stanza for the current
            # summary report. Since no plot files are created by this process, it can be skipped.
            if index == idx:
                continue

            # Determine the filename for this report's individual report.json file stored in the experiment
            # directory. Load the contents of that JSON file.
            report_file_path = self.experiment_manager.experiment_reports_dir_path / f"report_{index}.json"
            inputs.append(str(report_file_path))
            with open(report_file_path, 'r') as file:
                report_file_content = json.load(file)

            # Convert the loaded JSON content to a Prodict (to make retrieving attributes "cleaner").
            report_prodict = Prodict.from_dict(report_file_content["reports"][0])

            # Add any ROC plots to the summary file, if necessary.
            if report_prodict.fqcn == ReportType.PLOT_ROC:
                plot_file = report_prodict.kwargs.output_filename
                if plot_file not in plot_files:
                    plot_files.append(report_prodict.kwargs.output_filename)

            # Add any PR plots to the summary file, if necessary.
            elif report_prodict.fqcn == ReportType.PLOT_PR:

                # TODO: These names should come from the plotting curve code once we refactor jb_plot_pr
                plot_filenames = ["pr_curve.png", "pc_curve.png", "rc_curve.png"]

                for file_name in plot_filenames:
                    plot_file = str(Path(report_prodict.kwargs.output_dir) / file_name)
                    if plot_file not in plot_files:
                        plot_files.append(plot_file)

            # This is where you could do some work to include plot_files from other Report types, but
            # for now just continue on if the Report's fqcn is unfamiliar.
            else:
                continue

        # If any changes were made to the plot_files, the content for the JSON file should reflect
        # those changes.
        report_json_content.kwargs.plot_files = plot_files

        # Create the individual JSON file for this report and add the rule to the workflow.
        self._finalize_report(report_json_content, report_json_path, inputs, requirements, outputs)

    def add_generic_report(self, report: Report, idx: int) -> None:
        """
        This method is responsible for adding a rule to the workflow for all other report
        types that don't have a specific method defined for adding their workflow rules. The
        downside of this method is that due to the unknown nature of the report's inputs and
        outputs, those aspects of the report rule are difficult to fill out with any accuracy.
        :param report: One Report stanza from the list of reports described in the "reports" stanza
        of the experiment config. The 'fqcn' in this stanza will not match any of the known Report types.
        :param idx: An integer describing the position of the report parameter within the list of
        reports in the experiment config's "reports" stanza. Used to produce a unique filename.
        :return: Nothing.
        """
        # Set up some lists to track some information for pydoit.
        inputs = []
        requirements = []
        outputs = []

        # Use the idx to construct a unique filename for the individual JSON file that will contain
        # ONLY the information for this report.
        report_json_name = f"report_{idx}.json"
        report_json_path = self.experiment_manager.experiment_reports_dir_path / report_json_name

        # The individual JSON file for this report is a product of this process, so add it to the
        # list of outputs.
        outputs.append(str(report_json_path))

        # Initialize the contents of the JSON file to match the desired report.
        report_json_content = report

        # Create the individual JSON file for this report and add the rule to the workflow.
        self._finalize_report(report_json_content, report_json_path, inputs, requirements, outputs)

    def _finalize_report(self, json_content: dict, json_path: Path,
                         inputs: List, requirements: List, outputs: List) -> None:
        """
        This method is responsible for performing the final steps required to write the individual
        report JSON file and workflow rule.
        :param json_content: A dictionary corresponding to a single Report stanza. This content is to
        be written the the individual report.json file.
        :param json_path: The Path indicating where to save the current JSON file.
        :param inputs: The list of any input files for this workflow rule.
        :param requirements: The list of any requirement rules for this workflow rule
        :param outputs: The list of output files generated by this rule.
        :return: Nothing
        """

        # The "classes" and "tests" stanzas are not required in the Report's JSON file.
        if "classes" in json_content:
            json_content.pop("classes")
        if "tests" in json_content:
            json_content.pop("tests")

        # Format the contents of the JSON file correctly.
        json_content = {"reports": [json_content]}

        # Open the JSON file for writing and dump the content.
        with open(json_path, 'w') as file:
            json.dump(json_content, file, indent=4)
        logger.info(f"Saved individual report JSON to {json_path}")

        # Add the newly created JSON file to the list of outputs for this rule.
        outputs.append(str(json_path))

        # Group the lists into a tuple since that's the format the next method expects.
        workflow_components = (inputs, requirements, outputs)

        # Add the workflow rule.
        self.add_workflow_rule(workflow_components, json_path)

    def add_workflow_rule(self, workflow_components: tuple, report_file_path: Path) -> None:
        """
        This method is responsible for adding a rule to the workflow. The rule will run jb_report using an
        individual report.json file.
        :param workflow_components: A tuple of the form (inputs, requirements, outputs) where each component is
        a list.
        :param report_file_path: The Path to the individual report JSON file to be executed by 'jb_report'.
        :return: Nothing.
        """
        # Separate the workflow_components tuple into the individual lists.
        inputs, requirements, outputs = workflow_components

        # Remove any duplicate entries in the lists of inputs and outputs.
        inputs = list(set(inputs))
        outputs = list(set(outputs))

        # Build the command and doc string.
        experiment_log_dir = self.experiment_manager.experiment_log_dir_path
        command = [JB_REPORT_COMMAND, '-f', str(report_file_path), '-l', str(experiment_log_dir)]
        doc = " ".join([str(x) for x in command])

        clean_extras = []

        # Get the workflow and add the rule.
        workflow = self.builder.get_workflow("main")
        workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

    # ==============================================================================================

    #  _____ _ _ _
    # |  ___(_) | |_ ___ _ __ ___
    # | |_  | | | __/ _ \ '__/ __|
    # |  _| | | | ||  __/ |  \__ \
    # |_|   |_|_|\__\___|_|  |___/

    def add_filters(self, filters, model_mgr: ModelManager, dataset_path, cmd_id: int):
        for tag in filters:
            # Grab the filter from our map and fill in the entries.
            my_filter: Filter = self.filter_map.get(tag, None)
            if my_filter is None:
                logger.error(f"Can't find filter {tag} in filter list. Exiting.")
                sys.exit(-1)

            # Convert the replacement fields.
            command = [self.format_arg(i, model_mgr, dataset_path, tag) for i in my_filter.cmd]
            inputs = []
            if my_filter.inputs is not None:
                inputs = [self.format_arg(i, model_mgr, dataset_path, tag) for i in my_filter.inputs]
            outputs = []
            if my_filter.outputs is not None:
                outputs = [self.format_arg(i, model_mgr, dataset_path, tag) for i in my_filter.outputs]
            requirements = [cmd_id]

            # We are guessing here.
            doc = f"Filter - '{model_mgr.model_name}' with '{command[0]}'"
            clean_extras = []

            workflow = self.builder.get_workflow("main")
            workflow.add_rule(inputs, outputs, command, doc, clean_extras, requirements)

    @staticmethod
    def format_arg(arg: str, model_mgr: ModelManager, dataset_path: str, filter_tag: str):
        # This is basically just a glorified format string
        replace_map = {
            'model_name': Path(model_mgr.model_name),
            'train_output': Path(model_mgr.get_training_out_file())
        }
        if dataset_path is not None:
            replace_map['dataset_path'] = Path(dataset_path)
            replace_map['eval_predictions'] = model_mgr.get_eval_dir_mgr(dataset_path).get_predictions_path()

        # TODO: Add support for workspace and dataroot

        try:
            return arg.format(**replace_map)
        except KeyError:
            logger.error(f"Failed to convert the arg '{arg}' in filter '{filter_tag}' during replacement. Exiting.")
            sys.exit(-1)


# ==================================================================================================

def main():
    parser = argparse.ArgumentParser(description="Generates a rules list in the experiment directory from an "
                                                 "experiment config.")
    parser.add_argument('experimentName', help='Name of the experiment in the experiments directory.')

    args = parser.parse_args()

    maker = RuleMaker(args.experimentName)
    experiment_manager = ExperimentManager(args.experimentName)
    maker.save(experiment_manager.get_experiment_rules())

    logger.info(f"jb_experiment_to_rules is done.")


if __name__ == "__main__":
    main()
