#! /usr/bin/env python3

# ======================================================================================================================
# Juneberry - General Release
#
# Copyright 2021 Carnegie Mellon University.
#
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS"
# BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER
# INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED
# FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM
# FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
#
# Released under a BSD (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
#
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see
# Copyright notice for non-US Government use and distribution.
#
# This Software includes and/or makes use of Third-Party Software subject to its own license.
#
# DM21-0884
#
# ======================================================================================================================

import argparse
import csv
import logging
from pathlib import Path
import sys

from juneberry.config.eval_output import EvaluationOutput
from juneberry.config.training_output import TrainingOutput
from juneberry.filesystem import ModelManager
import juneberry.scripting as jbscripting

logger = logging.getLogger("juneberry.jb_summary_report")


def determine_shared_metric(prediction_file_list: list) -> str:
    """
    This function iterates through the list of specified predictions files and determines if
    the report is summarizing classification or object detection metrics. If a mismatch is
    detected, this function will terminate execution.
    :param prediction_file_list: A list of the predictions files that were provided as arguments
    to jb_summary_report.
    :return: A string ("Balanced Accuracy" | "mAP") indicating which metric the prediction files
    all have in common.
    """

    # Start with an empty list of metrics.
    from collections import defaultdict
    counts = defaultdict(list)

    # Loop through the list of predictions files and load the evaluation data.
    for prediction_file in prediction_file_list:
        eval_data = EvaluationOutput.load(prediction_file)

        # Identify the metrics.  In the case of the classification metrics, both may be there
        # so we need to count them.
        found = False
        if eval_data.results.metrics.accuracy is not None:
            counts['Acc'].append(prediction_file)
            found = True
        if eval_data.results.metrics.balanced_accuracy is not None:
            counts['BalAcc'].append(prediction_file)
            found = True
        if eval_data.results.metrics.bbox is not None:
            counts['mAP'].append(prediction_file)
            found = True
        if not found:
            counts['None'].append(prediction_file)

    # If we have some that we didn't identify, then show the files and bail.
    if len(counts['None']) > 0:
        logger.error(f"Found prediction files without known metrics: {counts['None']}. EXITING.")
        sys.exit(-1)

    # If we have Classification AND OD metrics at the same time, they are not comparable. Bail.
    if (len(counts['Acc']) > 0 or len(counts['BalAcc']) > 0) and len(counts['mAP']) > 0:
        logger.error(f"Found a mixture of classification and object detection metrics in the prediction files. "
                     f"Classification files={counts['Acc']} or {counts['BalAcc']}, "
                     f"object detection files={counts['mAP']}. EXITING.")
        sys.exit(-1)

    # If we have mAP use that, else figure out which classification is larger and use it.
    if len(counts['mAP']):
        metric = "mAP"
    elif len(counts['BalAcc']) > len(counts['Acc']):
        assert len(counts['BalAcc']) == len(prediction_file_list)
        metric = "Balanced Accuracy"
    else:
        assert len(counts['Acc']) == len(prediction_file_list)
        metric = "Accuracy"

    # If each prediction file contains the same type of metric, that metric must be the shared metric.
    logger.info(f"Building a report to summarize {metric}.")

    return metric


def retrieve_summary_data(model_mgr: ModelManager, prediction_data: EvaluationOutput, metric: str) -> list:
    """
    This function is responsible for retrieving the data that will be used to populate
    the Experiment Summary table in the summary report.
    :param model_mgr: A Juneberry model manager corresponding to the model whose training
    data is being added to the report.
    :param prediction_data: The data read in from the requested prediction file.
    :param metric: A string ('Balanced Accuracy' | 'mAP') indicating which metric is being
    retrieved for the summary report.
    :return: A list of the values that were retrieved for the summary report.
    """

    # Use the model manager to fetch the data from the model's output.json.
    train_data = TrainingOutput.load(model_mgr.get_training_out_file())

    # Obtain the desired metrics for the summary from the training and evaluation output.
    duration = train_data.times.duration
    name = prediction_data.options.model.name
    eval_data_name = prediction_data.options.dataset.config

    # Fetch the correct evaluation metric from the results.
    if metric == 'Accuracy':
        metric_value = prediction_data.results.metrics.accuracy
    elif metric == 'Balanced Accuracy':
        metric_value = prediction_data.results.metrics.balanced_accuracy
    elif metric == 'mAP':
        metric_value = prediction_data.results.metrics.bbox['mAP']
    else:
        metric_value = "N/A"

    return [name, duration, eval_data_name, metric_value]


def generate_summary_report(prediction_files: list, plot_files: list, summary_filename: str, csv_filename: str) -> None:
    """
    This function is responsible for producing the summary report file, and the summary
    CSV (if necessary).
    :param prediction_files: A list of the predictions files that were provided as arguments
    to jb_summary_report.
    :param plot_files: A list of the plot files that should be included in the summary report.
    :param summary_filename: A string indicating the filename for the summary report file.
    :param csv_filename: A string indicating the filename for the CSV file, if one was requested.
    :return: Nothing.
    """

    # TODO: Translate the variables in the model names (those that were
    #  generated during an experiment) into table columns.

    # Figure out which metric the prediction files have in common.
    metric = determine_shared_metric(prediction_files)

    # Generate the header of the summary report.
    header = f"# Experiment summary\nModel | Duration (seconds) | Eval Dataset | {metric} | " \
             f"Train Chart\n--- | --- | --- | --- | ---\n"

    # Begin writing the summary report to file.
    with open(summary_filename, "w") as outFile:

        # Establish the results list.
        results = [["model", "duration", "eval_dataset", {metric}]]

        # Add the header to the report file.
        outFile.write(header)

        # Loop through the prediction files.
        for file in prediction_files:

            logger.info(f"Adding the data from {file} to the summary report.")

            prediction_data = EvaluationOutput.load(file)
            model_name = prediction_data.options.model.name

            # Use the model name to create a model manager.
            model_mgr = ModelManager(model_name)

            # The goal is to obtain the location of the training summary plot relative to the location
            # of the summary file. Start by obtaining the absolute path of each file, break that path
            # down into its component parts, then convert those parts into a list.
            train_img_parts = list(model_mgr.get_training_summary_plot().resolve().parts)
            summary_file_parts = list(Path(summary_filename).resolve().parts)

            # Starting from index 0, the goal here is to determine how many of the component parts
            # match between the two files.
            similar_count = 0
            floor = min(len(train_img_parts), len(summary_file_parts))
            for i in range(0, floor):
                if train_img_parts[i] == summary_file_parts[i]:
                    similar_count += 1
                else:
                    break

            # Trim the similar components from the summary file.
            summary_file_parts = summary_file_parts[similar_count:]

            # Now construct a path to the training summary plot that is relative to the location of the
            # summary file. The remaining length of the summary file parts minus 1 (because you don't count
            # the summary file itself) indicates how many directories you need to "move back" before you find
            # a common parent between the training summary plot and the summary file. So prepend N number of
            # "../" strings to the location of the model's training summary plot.
            train_img_str = "../" * (len(summary_file_parts) - 1) + str(model_mgr.get_training_summary_plot())

            # Retrieve the summary data for this predictions file and append it to the results list.
            summary = retrieve_summary_data(model_mgr, prediction_data, metric)
            results.append(summary)

            # Write the retrieved data to the report file. The format for this line is different
            # depending on which metric is being summarized.
            if metric == 'Accuracy' or metric == 'Balanced Accuracy':
                outFile.write(f"{summary[0]} | {summary[1]} | {summary[2]} | {summary[3]:.2%} | "
                              f"[Training Chart]({train_img_str})\n")
            elif metric == 'mAP':
                outFile.write(f"{summary[0]} | {summary[1]} | {summary[2]} | {summary[3]} | "
                              f"[Training Chart]({train_img_str})\n")

        # Produce the CSV file (if necessary).
        if csv_filename is not None:
            with open(csv_filename, "w") as f:
                writer = csv.writer(f)
                writer.writerows(results)

        # Add any plot files to the report.
        if plot_files is not None and len(plot_files) > 0:
            outFile.write("\n# Experiment Plots\n")
            outFile.write("---\n".join([f"![Plot Image]({p.replace(' ', '%20')})\n" for p in plot_files]))


def setup_args(parser) -> None:
    """
    Adds arguments to the parser
    :param parser: The parser in which to add arguments.
    """
    parser.add_argument('-f', '--predictionFile', action='append', required=True,
                        help="The name of a file containing the prediction data to use. The expected format is "
                             "described in the predictions_specification. You can specify this argument multiple "
                             "times")
    parser.add_argument('-pf', '--plotFile', action='append', required=False,
                        help="Name of a Plot file that has been generated by the experiment.")
    parser.add_argument('summaryFilename', help="Markdown report will be written to this file")
    parser.add_argument('--csvFilename', default=None, required=False, help="CSV file to write summary table data")


def main():
    parser = argparse.ArgumentParser(description='This script generates a markdown summary report of an experiment.')
    setup_args(parser)
    jbscripting.setup_args(parser)
    args = parser.parse_args()

    # Set up logging.
    jbscripting.setup_logging_for_script(args)

    generate_summary_report(args.predictionFile, args.plotFile, args.summaryFilename, args.csvFilename)

    logger.info(f"jb_summary_report is done.")


if __name__ == "__main__":
    main()
