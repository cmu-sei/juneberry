#! /usr/bin/env python3

# ======================================================================================================================
# Juneberry - General Release
#
# Copyright 2021 Carnegie Mellon University.
#
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS"
# BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER
# INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED
# FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM
# FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
#
# Released under a BSD (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
#
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see
# Copyright notice for non-US Government use and distribution.
#
# This Software includes and/or makes use of Third-Party Software subject to its own license.
#
# DM21-0884
#
# ======================================================================================================================

import argparse
import logging
from pathlib import Path

from juneberry.config.model import Plugin
import juneberry.filesystem as jbfs
import juneberry.metrics.metrics_manager as metrics_manager
from juneberry.metrics.metrics_plot import PrecisionRecallPlot, PrecisionConfidencePlot, RecallConfidencePlot
import juneberry.scripting as jbscripting


logger = logging.getLogger("juneberry.jb_plot_pr")


def setup_args(parser) -> None:
    """
    Adds arguments to the parser
    :param parser: The parser in which to add arguments.
    """
    parser.add_argument('-m', '--model', action='append', required=True,
                        help="The name of a model to generate curves for. You may provide this argument any number of "
                             "times to add multiple curves to the figures, however each model argument must be paired "
                             "with a data argument. The figure will begin to repeat line colors if more than ten "
                             "curves are added to the plot.")
    parser.add_argument('-e', '--evalData', action='append', required=True,
                        help="The name of a dataset that was used to evaluate the model. In order for a curve to be "
                             "added to the figure, a model must be paired with a dataset.")
    parser.add_argument('--iou', type=float, default=0.5,
                        help="The IoU threshold to use when determining whether to count a detection as a true "
                             "positive.")
    parser.add_argument('--outputDir', default=Path.cwd(),
                        help="Directory where figures will be saved. If this field is left unspecified, the figures "
                             "will be saved to the current working directory.")


def main():
    parser = argparse.ArgumentParser(description='This script consumes the output from jb_evaluate to create '
                                                 'three figures: a Precision-Recall curve, a Precision-Confidence '
                                                 'curve, and a Recall-Confidence curve. A curve in each figure will be '
                                                 'generated for each pair of {model, eval datasets} provided in the '
                                                 'args to this script.')
    setup_args(parser)
    jbscripting.setup_args(parser)
    args = parser.parse_args()

    # Create the output directory if it does not already exist.
    output_dir = Path(args.outputDir)
    if not output_dir.exists():
        output_dir.mkdir(parents=True)

    # Set up logging.
    jbscripting.setup_logging_for_script(args)

    logger.info(f"Starting to generate PR, PC, and RC curves...")

    # Create empty MetricsPlots
    pr_plot = PrecisionRecallPlot()
    pc_plot = PrecisionConfidencePlot()
    rc_plot = RecallConfidencePlot()

    stats_fqcn = "juneberry.metrics.metrics.Stats"

    metrics_config = Plugin.from_dict({
        "fqcn": stats_fqcn,
        "kwargs": {
            "iou_threshold": args.iou,
            "tp_threshold": 0.8
        }
    })

    mm = metrics_manager.MetricsManager([metrics_config])

    for idx, val in enumerate(args.model):
        model_mgr = jbfs.ModelManager(args.model[idx])
        eval_dir_mgr = model_mgr.get_eval_dir_mgr(args.evalData[idx])

        metrics = mm.call_with_eval_dir_manager(eval_dir_mgr)[stats_fqcn]

        model_name = model_mgr.model_name
        dataset_name = eval_dir_mgr.get_dir().stem

        pr_plot.add_metrics(metrics, args.iou, model_name, dataset_name)
        pc_plot.add_metrics(metrics, args.iou, model_name, dataset_name)
        rc_plot.add_metrics(metrics, args.iou, model_name, dataset_name)

    pr_plot.save(output_dir / "pr_curve.png")
    pc_plot.save(output_dir / "pc_curve.png")
    rc_plot.save(output_dir / "rc_curve.png")

    # Export the metrics as a CSV
    # CommonMetrics.export(metrics, output_dir / "eval_metrics.csv")

    logger.info(f"jb_plot_pr is done.")


if __name__ == "__main__":
    main()
