#! /usr/bin/env python3

# ======================================================================================================================
# Juneberry - General Release
#
# Copyright 2021 Carnegie Mellon University.
#
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS"
# BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER
# INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED
# FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM
# FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
#
# Released under a BSD (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
#
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see
# Copyright notice for non-US Government use and distribution.
#
# This Software includes and/or makes use of Third-Party Software subject to its own license.
#
# DM21-0884
#
# ======================================================================================================================

"""
This script cleans all of the predictions files produced by jb_evaluate.

"""

import argparse
import logging
import subprocess

from juneberry.config.rule_list import RulesList
from juneberry.filesystem import ExperimentManager
import juneberry.scripting.utils as jb_scripting

logger = logging.getLogger("juneberry.jb_clean_experiment_evals")


def setup_args(parser) -> None:
    """
    Adds arguments to the parser
    :param parser: The parser in which to add arguments.
    """
    parser.add_argument("experimentName", help='Name of the experiment in the experiments directory whose eval '
                                               'directories should be cleaned.')


def main():
    parser = argparse.ArgumentParser(description="Cleans the eval directories in an experiment.")
    setup_args(parser)
    jb_scripting.setup_args(parser)
    args = parser.parse_args()

    experiment_manager = ExperimentManager(args.experimentName)
    log_file = experiment_manager.get_log_path()
    banner_msg = f">>> Juneberry Experiment Eval Cleaner - {args.experimentName} <<<"

    lab = jb_scripting.setup_workspace(args, log_file=log_file, log_prefix="<<LIVE>> ", banner_msg=banner_msg)
    workspace_root = lab.workspace()
    rules_file = experiment_manager.get_experiment_rules()
    dodo_file = experiment_manager.get_experiment_dodo(workflow="main")

    rules = RulesList.load(rules_file)

    tasks = []
    for workflow in rules.workflows:
        for rule in workflow.rules:
            if rule.command[0] == "jb_evaluate":
                tasks.append(rule.id)

    for task in tasks:
        cmd = ["doit", "-f", dodo_file, "--dir", workspace_root, "clean", str(task)]
        subprocess.run(cmd)

    logger.info("jb_clean_experiment_evals is done.")


if __name__ == "__main__":
    main()
