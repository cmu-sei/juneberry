#! /usr/bin/env python3

# ==========================================================================================================================================================
#  Copyright 2021 Carnegie Mellon University.
#
#  NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS"
#  BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER
#  INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED
#  FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM
#  FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT. Released under a BSD (SEI)-style license, please see license.txt
#  or contact permission@sei.cmu.edu for full terms.
#
#  [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see
#  Copyright notice for non-US Government use and distribution.
#
#  This Software includes and/or makes use of the following Third-Party Software subject to its own license:
#  1. Pytorch (https://github.com/pytorch/pytorch/blob/master/LICENSE) Copyright 2016 facebook, inc..
#  2. NumPY (https://github.com/numpy/numpy/blob/master/LICENSE.txt) Copyright 2020 Numpy developers.
#  3. Matplotlib (https://matplotlib.org/3.1.1/users/license.html) Copyright 2013 Matplotlib Development Team.
#  4. pillow (https://github.com/python-pillow/Pillow/blob/master/LICENSE) Copyright 2020 Alex Clark and contributors.
#  5. SKlearn (https://github.com/scikit-learn/sklearn-docbuilder/blob/master/LICENSE) Copyright 2013 scikit-learn
#      developers.
#  6. torchsummary (https://github.com/TylerYep/torch-summary/blob/master/LICENSE) Copyright 2020 Tyler Yep.
#  7. adversarial robust toolbox (https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/LICENSE)
#      Copyright 2018 the adversarial robustness toolbox authors.
#  8. pytest (https://docs.pytest.org/en/stable/license.html) Copyright 2020 Holger Krekel and others.
#  9. pylint (https://github.com/PyCQA/pylint/blob/master/COPYING) Copyright 1991 Free Software Foundation, Inc..
#  10. python (https://docs.python.org/3/license.html#psf-license) Copyright 2001 python software foundation.
#
#  DM20-1149
#
# ==========================================================================================================================================================

import re
import sys
import json
import logging
import argparse
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

import juneberry.scripting as jbscripting


def load_roc_data(filename):
    """
    This function is responsible for retrieving the required information from
    a ROC data file.
    :param filename: The file containing the data we want to extract.
    :return y_test: A numpy array containing the correct labels for the test images.
    :return y_score: A numpy array containing the predictions for all of the test images.
    :return mapping: A dictionary which translates label integers to labelNames.
    """
    # Open up the target JSON file and read in the data
    with open(filename, 'r') as in_file:
        data = json.load(in_file)

    # Retrieve the mapping dictionary.
    mapping = data['testOptions']['mapping']
    num_classes = data['testOptions']['numModelClasses']

    # Convert test_labels to y_test. Right now the roc_curve function cannot
    # process our test_labels as-is.
    labels = np.asarray(data['testResults']['labels'])
    y_test = np.zeros((labels.size, num_classes))
    for i in range(labels.size):
        j = labels[i]
        y_test[i][j] = 1

    y_score = np.asarray(data['testResults']['predictions'])

    return y_test, y_score, mapping


def compute_roc_data(test, score):
    """
    This function is responsible for retrieving the required information from
    a ROC data file.
    :param test: A numpy array containing correct labels for the images.
    :param score: A numpy array containing predictions for all of the test images
    :return fpr: A numpy array containing the fpr data used in the ROC curve.
    :return tpr: A numpy array containing the tpr data used in the ROC curve.
    :return roc_auc: A numpy array containing the area under the ROC curve.
    """

    # Compute the fpr and tpr data
    fpr, tpr, _ = roc_curve(test, score)

    # Compute the area under the ROC curve
    roc_auc = auc(fpr, tpr)

    return tpr, fpr, roc_auc


def get_label_count(file_list, classes_list):
    num_labels = 0
    for file in file_list:
        plot_arg = classes_list[file_list.index(file)]
        if plot_arg == 'all':
            y_test, y_score, mapping = load_roc_data(file)
            num_labels += len(mapping)
        else:
            num_labels += len(plot_arg.split(","))
    return num_labels


def mapping_lookup(string, mapping):
    """
    This function is responsible for looking up if a string is present in the values
    of a mapping dictionary. If the string is found, the function will return the matching
    key.
    :param string: The string to look up in the mapping dictionary.
    :param mapping: The mapping dictionary where the string might be found.
    :return: The key in the mapping where the value matches the search string.
    """
    if string in mapping.values():
        return next(key for key, value in mapping.items() if value == string)
    else:
        return None


def check_and_convert_classes(files: list, orig_class_list: list):
    """
    From the command line, the user can indicate which classes they want to plot either
    by using the integer for the class number or the human readable name. This function is
    responsible for converting any human readable names in the list to their integer
    counterpart. Integer form is required to perform the lookup in the predictions file.
    :param files: The list of predictions files being used in this plot.
    :param orig_class_list: The list of classes the user would like to plot from each
    prediction file. The classes will either be integers, a human readable string, or
    some combination of both.
    :return: A new list of classes, where all of the classes are now integers.
    """

    # Initialize blank lists for error messages and the converted class strings.
    error_messages = []
    new_class_list = []

    # Loop through each of the groups in the class list
    for group in orig_class_list:

        # Identify which file has the mapping for this group of classes
        current_file = files[orig_class_list.index(group)]

        # Load the mapping from the identified file
        with open(current_file, 'r') as prediction_file:
            mapping = json.load(prediction_file)['testOptions']['mapping']

        if group == 'all':
            new_class_list.append(group)
            continue

        # Split the class group based on our , and + delimiters
        split_group = re.split(',|\+', group)

        # Loop through each of the classes in the group
        for component in split_group:

            # Try to convert the class to an integer. If it's already an integer,
            # then there's nothing else we need to do.
            try:
                int(component)

            # When the class isn't an integer, look it up in the mapping.
            except ValueError:
                key = mapping_lookup(component, mapping)

                # If the component was found in the mapping, replace the component
                # string in the group with the key string
                if key is not None:
                    group = group.replace(component, key)

                # If the component wasn't in the mapping, create and store an error message.
                else:
                    message = f"Unknown class: \"{component}\" not found in mapping of {current_file}"
                    error_messages.append(message)

        # Add the group to the list of converted groups
        new_class_list.append(group)

    # If the process generated any error messages, log them and then exit.
    if len(error_messages):
        for message in error_messages:
            logging.error(message)
        sys.exit(-1)

    # If there weren't any errors, it is safe to return the list of converted strings.
    else:
        return new_class_list


def generate_roc_plot(file_list, classes_list, outfile, plot_title: str, legend_scaling):
    """
    This function is responsible for creating the ROC curve plot.
    :param file_list: A list of files containing the predictions we want to use
    for creating the ROC plot.
    :param classes_list: The list of labels in each file that we want to include on
    the plot. We're expecting comma separating strings of numbers (or label names) or the string 'all'.
    :param outfile: The filename we should use when saving the plot.
    :param plot_title: A string to use for the title of the plot
    :param legend_scaling: Scale factor to use for the legend.
    :return: Nothing.
    """

    # Set some parameters for the figure
    lw = 2  # line width
    fig = plt.figure(1)

    # IMPORTANT: The default resolution is 100 dpi so when we do things in inches we can use
    # decimals like 0.25 to be 25 pixels.

    # Apparently, matplotlib axes vs figure sizing is a problem. In general there are controls
    # for setting the figure size but not as many for the axis size. There are many posts about
    # this.

    # ROC curves are supposed to be square, so the first thing we do is set their aspect ratio
    # but also allow it to be adjustable.  Because of how the layout magic works, the anchor is
    # important to get more of the legend.
    plt.gca().set_aspect('equal', adjustable='box', anchor="N")

    # Now, we need to have some space for the legend at the bottom.  However, the figure adds
    # extra padding for some reason so the number of inches we set it to isn't really right.
    # When we add some, it adds some more for some unknown reasons. So the multiplier has been
    # generated empirically to look OK for 10 items. At some point it would be nice to understand
    # exactly the relationship to the size and the height and how matplotlib lays out the legend.
    # IF YOU CHANGE ROOT SIZE, CHANGE THE MAGIC
    magic = 0.25  # Good for 10 items or less into a roughly 6 inch plot
    # Scaling of 1.36 for 25 items results in magic of 0.34
    # Scaling of 1.12 for 50 items results in magic of 0.28
    magic = magic * legend_scaling

    logging.info(f"Classes - original form: {classes_list}")

    classes_list = check_and_convert_classes(file_list, classes_list)

    logging.info(f"Classes -  integer form: {classes_list}")

    # Figure out how many labels we have.
    num_labels = get_label_count(file_list, classes_list)

    # Size it to a reasonable size and add the magic buffer.
    fig.set_size_inches(w=7, h=5 + magic * num_labels)

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(plot_title)
    ax = plt.gca()

    # Loop through every file in the file list and generate the desired curves
    for file in file_list:

        # Get the matching list of classes to plot for this file
        plot_arg = classes_list[file_list.index(file)]

        logging.info(f"Plotting ROC curves for classes {plot_arg} using data in {file}")

        # Get the labels, predictions, and label mapping from the input file.
        y_test, y_score, mapping = load_roc_data(file)
        # Create the list of labels being plotted.
        if plot_arg == 'all':
            plot_list = []
            for label in mapping.keys():
                plot_list.append(label)
        else:
            # Convert the comma separating string of labels into a list
            plot_list = plot_arg.split(",")

        # Loop through each of the classes we want to plot
        for plot in plot_list:

            # If we're combining classes we need to treat the plot differently
            if "+" in plot:

                # Split the combined classes into individual components
                component_list = plot.split("+")

                # Create two np arrays to store the combined data. These two
                # empty arrays should be the size of a single column in the
                # test | score arrays. We also need to create a string for the legend.
                test_data = np.zeros(len(y_test[:, 0]))
                score_data = np.zeros(len(y_score[:, 0]))
                class_string = ''

                # For each individual component that we are combining in the plot
                for component in component_list:
                    # Add its contribution to the test data, score data, and string
                    test_data += y_test[:, int(component)]
                    score_data += y_score[:, int(component)]
                    class_string += mapping[component] + " + "

                # Trim the last three characters from the string (gets rid of
                # the last " + ".
                class_string = class_string[:-3]

            # If we're just plotting a single class, get the test data, score data,
            # and string for the desired class.
            else:
                test_data = y_test[:, int(plot)]
                score_data = y_score[:, int(plot)]
                class_string = mapping[plot]

            # Calculate the TPR, FPR, and AUC for this particular plot.
            tpr, fpr, roc_auc = compute_roc_data(test_data, score_data)

            # For the legend, we need to isolate which file the curve came from.
            file_string = file.split("/")[-1]

            # Plot the TPR, FPR, and associate the data with the correct label.
            ax.plot(fpr, tpr, lw=lw, label=f"{file_string} - class {class_string} (area = {roc_auc:0.2f})")

            ax.legend(loc="upper center", bbox_to_anchor=(0.5, -0.15))

    # Save the figure to the file
    plt.tight_layout()
    logging.info(f"Saving ROC curves to {outfile}")
    plt.savefig(outfile)


def setup_args(parser) -> None:
    """
    Adds arguments to the parser
    :param parser: The parser in which to add arguments.
    """
    parser.add_argument('-f', '--predictionFile', action='append', required=True,
                        help="The name of a file containing the prediction data to use. The expected format is "
                             "described in the predictions_specification. You can specify this argument multiple "
                             "times, but each instance should have its own plot argument.")
    parser.add_argument('-p', '--plotClasses', action='append', required=True,
                        help="The classes that should appear on the plot. This should either be the string "
                             "'all' (without quotes) or a comma separated string of class integers (or label names). "
                             "Using the 'all' string will plot every individual label present in the mapping of the "
                             "prediction file. Combining classes is supported and should be done with a '+'. "
                             "Example: 0,1,2,3,8,0+1,2+3 is a valid string. "
                             "Use the mapping portion of your predictionFile to determine which integers (or label "
                             "names) to use. Each 'f' argument should have one of these arguments.")
    parser.add_argument('--title', default='ROC - multiple classes',
                        help="This (optional) argument controls the title of the figure. When this argument is not "
                             "provided, a default title of \"ROC - multiple classes\" will be used.")
    parser.add_argument('--legendScaling', type=float, default=1.0,
                        help="The amount to scale the diagram for the legend. The default value (1.0) is "
                             "good for legends of roughly 10 items. Because matplotlib will automatically "
                             "try to scale the plot area, this number doesn't follow consistent rules. "
                             "For example, a scaling of 1.36 is good for around 25 items "
                             "while 1.12 is good for 50 items. You may need to fiddle with it a bit.")
    parser.add_argument('figureFilename', help="The name of the output file.")


def main():
    parser = argparse.ArgumentParser(description='This script creates a figure of one or more ROC curves using the '
                                                 'prediction data generated by a trained model.')
    setup_args(parser)
    jbscripting.setup_args(parser)
    args = parser.parse_args()

    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s', level=logging.INFO)

    # Some enforcement to make sure each file has a list of which classes to plot.
    if len(args.predictionFile) != len(args.plotClasses):
        logging.error("Number of predictionFile args does not match number of plotClasses args. Each "
                      "predictionFile should have a string of classes to plot.")
        sys.exit(-1)

    # Some enforcement to make sure each prediction file in the list is unique
    if len(args.predictionFile) != len(set(args.predictionFile)):
        logging.error("Duplicates detected in the list of predictionFiles. You should only "
                      "specify a predictionFile once. Consider combining the class strings.")
        sys.exit(-1)

    generate_roc_plot(args.predictionFile, args.plotClasses, args.figureFilename, args.title, args.legendScaling)

    logging.info("Done")


if __name__ == "__main__":
    main()
