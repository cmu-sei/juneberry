#! /usr/bin/env python3

# ======================================================================================================================
# Juneberry - General Release
#
# Copyright 2021 Carnegie Mellon University.
#
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS"
# BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER
# INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED
# FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM
# FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
#
# Released under a BSD (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
#
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see
# Copyright notice for non-US Government use and distribution.
#
# This Software includes and/or makes use of Third-Party Software subject to its own license.
#
# DM21-0884
#
# ======================================================================================================================

import argparse
from copy import deepcopy
import datetime
import itertools
import logging
import os
from prodict import Prodict
import random
import sys

import juneberry.config.experiment as jb_experiment
from juneberry.config.experiment import ExperimentConfig, ReportFQCN
import juneberry.config.experiment_outline as jb_exp_outline
from juneberry.config.experiment_outline import ExperimentOutline
from juneberry.config.model import ModelConfig
from juneberry.config.plugin import Plugin
from juneberry.filesystem import ExperimentCreator, ModelManager, save_json
import juneberry.scripting as jbscripting

logger = logging.getLogger("juneberry.jb_generate_experiments")


def build_tests(models: list, in_report: Prodict, out_report: Plugin) -> list:
    """
    This function is responsible for populating a Report's 'tests' section when the
    Report type is for a combined 'PR' or 'ROC' report.
    :param models: A list of every model in the experiment.
    :param in_report: A dictionary containing the parameters of the ROC | PR report we'd like to generate.
    :param out_report: The current contents of the output Report, with everything filled out but the
    'tests' section.
    return: A list with a single element, the output report.
    """

    # Start with an empty list for the 'tests' section.
    out_report.tests = []

    # Loop through every model in the list, build an appropriately named test tag, and
    # then append that test to the 'tests' section in the output Report.
    for model in models:
        test = {'tag': f"{in_report.test_tag}_{model}"}
        out_report.tests.append(jb_experiment.ReportTest.from_dict(test))

    # Return the output Report in the expected format.
    return [out_report]


def generate_combined_roc(in_report: Prodict, models: list) -> list:
    """
    This function is responsible for converting a report type of 'all_roc' into a 'plot_roc'
    that will plot all of the ROC curves for a particular test onto a single graph.
    :param in_report: A dictionary containing the parameters of the ROC report we'd like to generate.
    :param models: A list of every model in the experiment.
    :return: A list containing a ROC report dictionary that defines the combined plot.
    """

    # Create a Plugin for the new Report and fill in some fields from the input Report.
    out_report = Plugin()
    out_report.classes = in_report.classes
    out_report.description = in_report.description
    out_report.fqcn = ReportFQCN.PLOT_ROC.value

    # Retain the kwargs in the new Report if any were provided in the input Report.
    out_report.kwargs = in_report.kwargs if in_report.kwargs is not None else {}

    # Add some kwargs that are specific to this type of Report.
    out_report.kwargs.output_filename = f"{in_report.test_tag}_all_combined.png"
    out_report.kwargs.plot_title = f"ROC - All Tests Combined"

    # Use the 'build_tests' function to build an appropriate tag name for each model
    # and add each one to the 'tests' property of the output Report. Log the Report
    # that was created and return the Report.
    logger.info(f"Added 1 combined plot_roc report for tag '{in_report['test_tag']}' to the experiment config.")
    return build_tests(models, in_report, out_report)


def generate_roc_reports(in_report: Prodict, models: list) -> list:
    """
    This function is responsible for taking a report of type 'plot_roc' and applying
    the desired report parameters to each possible model in the experiment.
    :param in_report: A dictionary containing the parameters of the ROC report we'd like to run for every model.
    :param models: A list of the every model in the experiment.
    :return: A list of ROC report dictionaries; one for each model in the experiment.
    """
    roc_report_list = []

    # Loop over every model in the experiment
    for model in models:
        # Build a new Plugin for the output Report and fill in some fields
        # from the input Report.
        out_report = Plugin()
        out_report.classes = in_report.classes
        out_report.description = in_report.description
        out_report.fqcn = in_report.fqcn
        out_report.kwargs = in_report.kwargs if in_report.kwargs is not None else {}

        # Generate an appropriate tag for each model and add it to the 'tests'
        # property of the output report.
        tag = f"{in_report.test_tag}_{model}"
        out_report.tests = [jb_experiment.ReportTest(tag=tag)]

        # Set an appropriate output filename and plot title for each model.
        out_report.kwargs.output_filename = f"{in_report.test_tag}_{model}.png"
        out_report.kwargs.plot_title = f"ROC - {model}"

        # Append the dictionary to the list of reports
        roc_report_list.append(out_report)

    logger.info(f"Added {len(roc_report_list)} plot_roc reports for tag '{in_report['test_tag']}' "
                f"to the experiment config.")

    return roc_report_list


def generate_combined_pr(in_report: Prodict, models: list) -> list:
    """
    This function is responsible for converting a report type of 'all_pr' into a 'plot_pr'
    that will plot all of the PR curves for a particular test onto a single graph.
    :param in_report: A dictionary containing the parameters of the PR report we'd like to generate.
    :param models: A list of every model in the experiment.
    :return: A list containing a PR report dictionary that defines the combined plot.
    """

    # Build a new Plugin for the output Report and fill in some fields
    # from the input Report.
    out_report = Plugin()
    out_report.description = in_report.description
    out_report.fqcn = ReportFQCN.PLOT_PR.value
    out_report.kwargs = in_report.kwargs if in_report is not None else {}

    # Set an appropriately named output directory based on the test tag.
    out_report.kwargs.output_dir = f"{in_report.test_tag}_all_combined"

    # Use the 'build_tests' function to build an appropriate tag name for each model
    # and add each one to the 'tests' property of the output Report. Log the Report
    # that was created and return the Report.
    logger.info(f"Added 1 combined plot_PR report for tag '{in_report['test_tag']}' to the experiment config.")
    return [build_tests(models, in_report, out_report)]


def generate_pr_reports(in_report: Prodict, models: list) -> list:
    """
    This function is responsible for taking a report of type 'plot_pr' and applying
    the desired report parameters to each possible model in the experiment.
    :param in_report: A dictionary containing the parameters of the PR report we'd like to run for every model.
    :param models: A list of the every model in the experiment.
    :return: A list of PR report dictionaries; one for each model in the experiment.
    """
    pr_report_list = []

    # Loop through every model in the model list.
    for model in models:
        # Build a new Plugin for the output Report and fill in some fields
        # from the input Report.
        out_report = Plugin()
        out_report.description = in_report.description
        out_report.fqcn = in_report.fqcn
        out_report.kwargs = in_report.kwargs if in_report.kwargs is not None else {}

        # Generate an appropriate tag for each model and add it to the 'tests'
        # property of the output report.
        tag = f"{in_report.test_tag}_{model}"
        out_report.tests = [jb_experiment.ReportTest(tag=tag)]

        # Set an appropriate output directory that's based on the tag and model name.
        out_report.kwargs.output_dir = f"{in_report.test_tag}_{model}"

        # Append the current output report to the list of PR Reports.
        pr_report_list.append(out_report)

    # Log how many PR Reports were generated and return the list.
    logger.info(f"Added {len(pr_report_list)} plot_PR reports for tag '{in_report['test_tag']}' "
                f"to the experiment config.")

    return pr_report_list


def generate_reports_list(outline: ExperimentOutline, model_names: list) -> list:
    """
    This function is responsible for generating the list of reports that will go into the
    experiment config.
    :param outline: The experiment outline that describes the details of the experiment config.
    :param model_names: A list of all the model names in the experiment.
    :return: A list of all the reports that should be included in the experiment config.
    """

    logger.info(f"********** REPORTS **********")

    reports_list = []

    # We need to work on a copy of the outline, otherwise any changes made will
    # propagate to subsequent models.
    # report_outline = deepcopy(outline.reports)

    outline_reports = outline.reports

    # Loop over every report in the report outline
    for report in outline_reports:

        # If the report is for a ROC plot, we want to generate a version of that
        # report for every model in the experiment.
        if report.fqcn == ReportFQCN.PLOT_ROC:
            reports_list.extend(generate_roc_reports(report, model_names))

        elif report.fqcn == ReportFQCN.ALL_ROC:
            reports_list.extend(generate_combined_roc(report, model_names))

        elif report.fqcn == ReportFQCN.PLOT_PR:
            reports_list.extend(generate_pr_reports(report, model_names))

        elif report.fqcn == ReportFQCN.ALL_PR:
            reports_list.extend(generate_combined_pr(report, model_names))

        # If the report type is a summary , we only need one and can use the
        # parameters as-is.
        elif report.fqcn == ReportFQCN.SUMMARY:
            reports_list.append(report)
            logger.info(f"Added 1 summary report for this experiment.")

        # If the report fqcn wasn't recognized, add it as-is to the experiment config.
        else:
            reports_list.append(report)
            logger.info(f"Added 1 report with 'fqcn' {report.fqcn} to the experiment config.")

    logger.info(f"Added {len(reports_list)} total reports to the experiment config.")

    return reports_list


def generate_models_list(outline: ExperimentOutline, experiment_name: str, model_names: list) -> list:
    """
    This function is responsible for generating the list of models that will go into the
    experiment config.
    :param outline: The experiment outline that describes the details of the experiment config.
    :param experiment_name: A string equal to the name of the experiment.
    :param model_names: A list of all the model names in the experiment.
    :return:
    """

    logger.info(f"********** MODELS **********")

    models_list = []

    # Loop over every model in the experiment
    for model in model_names:

        # We need to work on a copy of the tests in the outline, otherwise changes
        # will propagate to subsequent models.
        tests = deepcopy(outline.tests)

        # Combine the name of the experiment and the name of the current model.
        name = experiment_name + "/" + model

        # Build a dictionary for the model using the unique name and a copy of all
        # the tests.
        out_model = jb_experiment.Model(name=name, tests=tests)

        # For each test, we want to make sure the tag is updated to reflect the
        # current model name and that a classify flag is present.
        for test in out_model.tests:
            test.tag += "_" + model
            if 'classify' not in test:
                test['classify'] = 0

        # If they specified filters then add them to the model
        if outline.model is not None:
            out_model.filters = outline.model.filters

        # Add the current model dictionary to the overall model list.
        models_list.append(out_model)

    logger.info(f"Added {len(models_list)} total models to the experiment config.")

    return models_list


def generate_experiment_config(experiment_creator: ExperimentCreator, outline: ExperimentOutline,
                               experiment_name: str, model_names: list) -> None:
    """
    This function is responsible for producing the content that will go into the config.json
    for this experiment and then saving it to disk.
    :param experiment_creator:
    :param outline:
    :param experiment_name:
    :param model_names:
    :return:
    """

    logger.info(f"Attempting to create a config.json for {experiment_name}...")

    # Generate the dictionary that will go into config.json
    out_config = jb_experiment.ExperimentConfig()
    out_config.description = "Generated using jb_generate_experiments"
    out_config.models = generate_models_list(outline, experiment_name, model_names)
    out_config.reports = generate_reports_list(outline, model_names)
    out_config.format_version = ExperimentConfig.FORMAT_VERSION
    out_config.timestamp = str(datetime.datetime.now().replace(microsecond=0).isoformat())
    out_config.filters = outline.filters

    # Get the location where we should save the config information.
    exp_config_file = experiment_creator.get_experiment_config()

    # Write the config content to the config file.
    out_config.save(exp_config_file)

    logger.info(f"Experiment config for {experiment_name} written to {exp_config_file}.")


def get_combo_name(chosen: dict, possible_options: list) -> str:
    """
    This function is responsible for deriving the correct name of the combination
    based on which properties were selected. A combination name takes the form
    A_B_C_..., where there will be one "slot" in the combo name for each possible variable
    in the config. A, B, C, etc. are integers which indicate the index position of the option
    that is present in the config for that variable. For example, a combo name of 2_1_0
    has three variables, and its config.json was built using variables A[2], B[1], and C[0].
    :param chosen: A dictionary that contains the variables that were selected for this combination.
    :param possible_options: A list that contains entries of all possible values for the variables. The
    variables in the "chosen" dictionary should all have come from these lists.
    :return: The name of the combination, as a string
    """

    # Initialize an empty combo name
    combo_name = ""

    # For each selection in the combo, the goal is to identify the index position
    # in the list for that variable and use that position in the combo name.

    # For each key in the combination to be implemented
    for key in chosen:
        # The modded key is necessary to account for cases where the target variable is a
        # particular element in an array.
        modded_key = None
        if "[" in key:
            modded_key = str(key).split("[")[0]

        # Locate the variable in the list
        option: jb_exp_outline.Variable
        for option in possible_options:
            if option.config_field == key or option.config_field == modded_key:

                # Add the nickname in the combo_name if there is one
                if option.nickname:
                    combo_name += option.nickname + "_" + str(option.vals.index(chosen[key])) + "_"
                else:
                    combo_name += str(option.vals.index(chosen[key])) + "_"

                # We've identified the correct variable, so move on to the next one.
                continue

    # Trim the last underscore and return the name
    return combo_name[:-1]


def apply_nested_parameter(config: dict, parameter: str, value) -> dict:
    """
    This function is responsible for applying a variable to a nested parameter
    in a dictionary representation of a ModelConfig object. It works by traversing the
    ModelConfig dictionary until it reaches the desired location and then applies the variable value.
    :param config: The dictionary representation of the ModelConfig object to modify.
    :param parameter: The nested parameter to modify.
    :param value: The value to apply to the nested parameter.
    :return: The modified dictionary representation of the ModelConfig object.
    """

    # Establish position at the "root" of the ModelConfig dictionary.
    cur_key = config

    # Split up the nested parameter into its component parts.
    components = parameter.split(".")

    # Traverse the dictionary, stopping at the final component.
    for component in components[:-1]:

        # If there's a bracket in the component name, that indicates a particular element of
        # an array is being targeted. The steps to set the cur_key are a little different in
        # this case to account for the targeted element in the array.
        if "[" in component:
            idx = int(component.split("[")[1].split("]")[0])
            component = component.split("[")[0]
            cur_key = cur_key[component][idx]
            continue

        if not isinstance(cur_key, dict) or component not in cur_key:
            logger.error(f"Failed to find '{parameter}' in config. Check parameter validity. Exiting.")
            sys.exit(-1)
        cur_key = cur_key[component]

    # Set the value of the final component to the desired variable value.
    cur_key[components[-1]] = value

    return config


def apply_parameter(config: dict, parameter: str, value) -> dict:
    """
    This function is responsible for applying the value of a variable to the
    appropriate parameter in a dictionary representation of a ModelConfig object. There are a few
    special cases that must be handled, such as nested parameters and parameters that should
    be grouped together.
    :param config: A dictionary representation of a ModelConfig object whose properties will be changed.
    :param parameter: The parameter in the ModelConfig that is being changed.
    :param value: The value that the parameter in the ModelConfig is being changed to.
    :return: The modified dictionary representation of the ModelConfig object.
    """

    if "seed" in parameter and value == "RANDOM":
        random.seed()
        value = random.randint(0, 2 ** 32 - 1)

    # "Comma notation" is how we handle parameters that need to be applied together. First we
    # must isolate the grouped parameters into individual parameters.
    if "," in parameter:
        components = parameter.split(",")

        # Once the parameters have been isolated, apply each parameter individually to the same config.
        for component in components:
            config = apply_parameter(config, component, value[component])

    # "Dot notation" is how we handle nested parameters. We'll call a separate function to handle that.
    # "Bracket notation" can also be handled by this same nested process.
    elif "." or "[" in parameter:
        config = apply_nested_parameter(config, parameter, value)

    # If the parameter isn't grouped or nested, it's "normal" so we can set it just like it's a
    # standard dictionary.
    else:
        config[parameter] = value

    return config


def check_combinations(variables: list, combos: list) -> None:
    """
    The purpose of this function is to make sure that every combination generated by this outline
    made a selection from each possible variable.
    :param variables: A list of the variable names we can change in a combination.
    :param combos: A list of all the combinations produced by this experiment outline.
    :return:
    """

    # Loop through all of the combinations
    for combo in combos:

        # Check if we have the same number of parameters in the combination as the number of possible variables.
        if len(variables) != len(combo):
            logger.error(f"Mismatch between the number of variable names and "
                         f"the number of variables in the combination for combo {combo}.")
            sys.exit(-1)


def validate_variables(variables: list) -> None:
    """
    Check to make sure the variable structure is valid.
    * config_field
    * nickname
    * vals
    :param variables: The list of variables to check
    :return: None
    """
    error_count = 0
    for variable in variables:
        if type(variable.vals) is list:
            fields = variable.config_field.split(',')
            if len(fields) > 1:
                # If we have multiple config fields, make sure that the keys exist in every entry.
                for i, val in enumerate(variable.vals):
                    val_keys = list(val.keys())
                    for key in fields:
                        if key not in val:
                            logger.error(
                                f"Key {key} not found in entry {i} in '{variable.config_field}'")
                            error_count += 1
                        else:
                            val_keys.remove(key)

                    # If any keys remain then they were not in the config.
                    if len(val_keys) > 0:
                        logger.error(f"Entry {i} in '{variable.config_field}' has extra keys {val_keys}!")
                        error_count += 1

    if error_count > 0:
        error_str = "error" if error_count == 1 else "errors"
        logger.error(f"Found {error_count} {error_str} when validating variables. Exiting.")
        sys.exit(-1)


def generate_combinations(variables: list) -> (list, list, list):
    """
    This functions is responsible for generating all possible combinations of variables
    that we want to apply to a ModelConfig.
    :param variables: This is a list of all the variables from a model outline. Each variable
    is a dictionary with keys describing the properties of the variable.
    :return every_variable_name: A list of which ModelConfig properties will be varied.
    :return every_variable_nickname: A list of nicknames for each variable.
    :return combinations: A list of every possible variable combination.
    """
    every_variable_name = []
    every_variable_list = []
    every_nickname_list = []

    # For each config property being changed, we want to separately capture the name of
    # the variable and the values that should be substituted in for that property.
    for variable in variables:
        if type(variable.vals) is list:
            every_variable_name.append(variable.config_field)
            every_nickname_list.append(variable.nickname)
            every_variable_list.append(variable.vals)

    # This itertools.product does the heavy lifting to create a list of every possible
    # combination of the variables. Example: If we have 3 variables and each variable
    # has X, Y, and Z options (respectively), then there should be X*Y*Z elements in the
    # combinations list.
    combinations = list(itertools.product(*every_variable_list))

    return every_variable_name, every_nickname_list, combinations


def generate_model_configs(experiment_outline: ExperimentOutline) -> list:
    """
    This function is responsible for producing the various combinations of
    ModelConfig objects and saving them to the appropriate JSON file.
    :param experiment_outline: An experiment outline object which contains all of the information
    required to build the model configs for the experiment.
    :return: A list of model names for the configs that were produced
    """

    # Load the baseline model config
    model_manager = ModelManager(experiment_outline.baseline_config, "")
    baseline_config = ModelConfig.load(model_manager.get_model_config())

    # Now we're going to do some work with the variables in the experiment.
    variables = experiment_outline.variables

    # Check to make sure the variables are well formed.
    validate_variables(variables)

    # We want to remove any variables that don't have a list of options so that they
    # don't affect the model name we're going to derive.
    withheld_variables = []
    for variable in list(variables):
        if type(variable.vals) is not list:
            withheld_variables.append(variable)
            variables.remove(variable)

    # Convert the variables into a list of variable names and a list of all
    # possible combinations of variables.
    variable_names, variable_nicknames, combinations = generate_combinations(variables)

    # Confirm the combinations are valid
    check_combinations(variable_names, combinations)

    # This tracks the number of configs that were produced
    model_names = []

    # Loop through every possible combination
    for combo in combinations:

        # Create a dictionary of the changes to apply to the baseline config, get the
        # combo name for those changes, and create a model manager for this combo.
        config_changes = dict(zip(variable_names, combo))
        combo_name = get_combo_name(config_changes, variables)
        model_manager = ModelManager(os.path.join(experiment_outline.experiment_name, combo_name), "")
        model_dir = model_manager.get_model_dir()

        # Create the directory for this combination if it doesn't exist.
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)

        # Add the withheld variables back to the changes being applied
        for variable in list(withheld_variables):
            config_changes[variable.config_field] = variable.vals

        # Initialize a new config using the baseline
        new_config = deepcopy(baseline_config).to_json()

        # Apply each of the changes to the new config
        for parameter in config_changes:
            new_config = apply_parameter(new_config, parameter, config_changes[parameter])

        # Save the summary info to the model
        new_config['summary_info'] = dict(zip(variable_nicknames, combo))

        # Save the modified ModelConfig to a JSON file in the appropriate directory.
        save_json(new_config, model_manager.get_model_config())

        # Add the name of the combo we created to the tracking list.
        model_names.append(combo_name)

    return model_names


def setup_args(parser) -> None:
    """
    Adds arguments to the parser
    :param parser: The parser in which to add arguments.
    """
    parser.add_argument('experimentName', help='Name of the experiment directory to generate experiment files for.')
    parser.add_argument('--dryrun', default=False, action='store_true',
                        help='Flag to initiate dry run mode. ')


def main():
    # Setup and parse all arguments.
    parser = argparse.ArgumentParser(description="Creates all combinations of model configs required to "
                                                 "run an experiment. Also creates the experiment config file "
                                                 "which can run the full experiment.")
    setup_args(parser)
    jbscripting.setup_args(parser)
    args = parser.parse_args()
    experiment_creator = ExperimentCreator(args.experimentName)

    # Set up the workspace, logging and general environment
    lab = jbscripting.setup_workspace(
        args,
        log_file=experiment_creator.get_log_path(args.dryrun),
        log_prefix=jbscripting.standard_line_prefix(args.dryrun))

    # Initial log message for this operation
    logger.info("Creating the experiment files for the following experiment: " + args.experimentName)

    # Load the experiment definition file
    logger.info(f"Loading experiment definition file...")
    experiment_outline = ExperimentOutline.load(experiment_creator.get_experiment_outline(), args.experimentName)

    # Perform a dry run if the option was provided
    if args.dryrun:
        experiment_outline.analyze_experiment_variables()
        variables, nicknames, combos = generate_combinations(experiment_outline.variables)
        check_combinations(variables, combos)

    # If it's not a dry run, then create the configs
    else:
        # Identify the config directory and create it if it doesn't exist
        config_dir = experiment_creator.experiment_model_dir_path
        if not os.path.exists(config_dir):
            os.makedirs(config_dir)

        # Generate the config files
        model_name_list = generate_model_configs(experiment_outline)
        logger.info(f"Created {len(model_name_list)} config files in {config_dir}")

        # Create the config.json for this experiment
        generate_experiment_config(experiment_creator, experiment_outline,
                                   experiment_creator.experiment_name, model_name_list)

    logger.info("Done!")


if __name__ == "__main__":
    main()
