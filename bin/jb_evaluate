#! /usr/bin/env python3

# ======================================================================================================================
# Juneberry - General Release
#
# Copyright 2021 Carnegie Mellon University.
#
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS"
# BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER
# INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED
# FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM
# FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
#
# Released under a BSD (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
#
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see
# Copyright notice for non-US Government use and distribution.
#
# This Software includes and/or makes use of Third-Party Software subject to its own license.
#
# DM21-0884
#
# ======================================================================================================================

import argparse
import logging
import sys
from types import SimpleNamespace

from juneberry.config.model import ModelConfig
import juneberry.evaluation.utils as jb_eval_utils
import juneberry.jb_logging as jblogging
import juneberry.filesystem as jbfs
import juneberry.platform_info
import juneberry.scripting as jbscripting

logger = logging.getLogger("juneberry.jb_evaluate")


def setup_eval_options(top_k=0, use_train_split=False, use_val_split=False):
    """
    This function converts a set of input arguments into a SimpleNameSpace representing the selected evaluator options.
    :param top_k: An integer that controls how many of the top-K predicted classes to record in the output.
    :param use_train_split: Flag that indicates if the training portion of the dataset should be evaluated.
    :param use_val_split: Flag that indicates if the validation portion of the dataset should be evaluated.
    :return: eval_options: A SimpleNamespace representing the Evaluator options that were set via
    command line arguments.
    """

    if use_train_split and use_val_split:
        logger.error("--useTrainSplit and --useValSplit should not be used together. Choose one and try again. "
                     "Exiting.")
        sys.exit(-1)

    # Set up a SimpleNamespace to capture the relevant evaluation options
    eval_options = SimpleNamespace()
    eval_options.top_k = top_k
    eval_options.use_train_split = use_train_split
    eval_options.use_val_split = use_val_split

    return eval_options


def setup_args(parser) -> None:
    """
    This function contains the arguments for the ArgumentParser that are unique to this script.
    :param parser: The ArgumentParser managing the arguments for this script.
    :return: Nothing
    """
    parser.add_argument('modelName', help='Name of a model directory containing the model you\'d like to evaluate.')

    parser.add_argument('dataset', type=str, default=[], nargs='*',
                        help="Optional Path to a dataset file describing the data the model should evaluate. Not "
                             "all evaluators will require this argument, but some do.")

    parser.add_argument('--dryrun', default=False, action='store_true', help='Flag to initiate dry run mode.')

    parser.add_argument('--useTrainSplit', default=False, action='store_true',
                        help='This argument reads the validation split parameters from the model\'s training config, '
                             'splits the dataset in the dataset arg according to those parameters, and then evaluates '
                             'the training portion of the split data. '
                             'This argument is incompatible with --useValSplit.')

    parser.add_argument('--useValSplit', default=False, action='store_true',
                        help='This argument reads the validation split parameters from the model\'s training config, '
                             'splits the dataset in the dataset arg according to those parameters, and then evaluates '
                             'the validation portion of the split data. '
                             'This argument is incompatible with --useTrainSplit.')

    parser.add_argument("--topK", default=0, type=int,
                        help='Optional parameter. An integer that controls how many of the top-K predicted '
                             'classes to record in the output.')

    parser.add_argument('-n', '--num-gpus', type=int, default=None,
                        help='The number of gpus. By default (unset) use all. Set to 0 for cpu.')

    parser.add_argument('--skip-exists', default=False, action="store_true",
                        help='If set to true, will skip the evaluation if a predictions file exists.')


def main():
    # Typical argparse operations to gather the command line arguments.
    parser = argparse.ArgumentParser(description='Loads a model, evaluates against a dataset, and saves results '
                                                 'to a predictions file.')
    setup_args(parser)
    jbscripting.setup_args(parser)
    args = parser.parse_args()

    # We may have any number of datasets. If we have more than one, use the first for initializing the model.
    first_dataset_name = args.dataset[0] if len(args.dataset) > 0 else None

    # Process arguments
    model_name = args.modelName
    model_manager = jbfs.ModelManager(model_name)
    eval_dir_mgr = model_manager.get_eval_dir_mgr(first_dataset_name)
    eval_dir_mgr.setup()
    eval_options = setup_eval_options(args.topK, args.useTrainSplit, args.useValSplit)

    # Convert verbose argument to proper logging level.
    level = logging.DEBUG if args.verbose else logging.INFO

    # Set up logging for the evaluation
    log_file = eval_dir_mgr.get_log_path(args.dryrun)
    lab = jbscripting.setup_workspace_and_model(
        args,
        log_file=log_file,
        log_prefix=jbscripting.standard_line_prefix(args.dryrun),
        model_name=model_name,
        banner_msg=">>> Juneberry Evaluator <<<")

    # Show the abbreviated platform info
    logger.info(f"Platform Info: {juneberry.platform_info.make_minimum_report()}")

    # Load the dataset.
    dataset = lab.load_dataset_config(first_dataset_name) if first_dataset_name is not None else None

    # Start evaluation
    logger.info(f"Script arguments captured. Ready to start an evaluation run.")

    # Load the model config in order to determine the type of evaluator needed
    # for this evaluation.
    model_config = ModelConfig.load(model_manager.get_model_config())

    # Set up the lab profile
    lab.setup_lab_profile(model_name=model_name, model_config=model_config)
    logger.info(f"Using lab profile: {lab.profile}")

    evaluator = jb_eval_utils.create_evaluator(model_config, lab, model_manager, eval_dir_mgr, dataset, eval_options,
                                               log_file)

    if evaluator is None:
        logger.error(f"No Evaluator instantiated. Exiting.")

    else:
        if args.dryrun:
            # Perform the dry run for the Evaluator.
            evaluator.dry_run()
            # TODO: Add dry run support for additional evals

        else:
            # Perform the evaluation.
            evaluator.num_gpus = evaluator.check_gpu_availability(lab.profile.num_gpus)
            if eval_dir_mgr.predictions_exists() and args.skip_exists:
                logger.info(f"--skip-exists is true and {eval_dir_mgr.get_predictions_path()} found. Skipping.")
            else:
                evaluator.perform_evaluation()

            # EXPERIMENTAL
            if len(args.dataset) > 1:
                for dsname in args.dataset[1:]:
                    # Try to reset the dataset file, reinit logging, and attempt another eval.
                    eval_dir_mgr = model_manager.get_eval_dir_mgr(dsname)
                    eval_dir_mgr.setup()
                    jblogging.setup_logger(log_file=eval_dir_mgr.get_log_path(args.dryrun),
                                           log_prefix=jbscripting.standard_line_prefix(args.dryrun),
                                           log_to_console=False,
                                           level=level)

                    dataset = lab.load_dataset_config(dsname)
                    if eval_dir_mgr.predictions_exists() and args.skip_exists:
                        logger.info(f"--skip-exists is true and {eval_dir_mgr.get_predictions_path()} found. Skipping.")
                    else:
                        evaluator.perform_additional_eval(eval_dir_mgr.get_log_path(args.dryrun), dataset, eval_dir_mgr)

            logger.info(f"Evaluation complete.")


if __name__ == "__main__":
    jbscripting.run_main(main, logger)
