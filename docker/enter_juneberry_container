#! /usr/bin/env bash

# =================================================================================================
# WARNING: These containers and scripts create containers with NO SECURITY PRACTICES, such as
# separate user accounts, unprivileged users, etc.
#
# USE AT YOUR OWN RISK
# =================================================================================================

# This is a sample startup script for the Juneberry container.  Please feel free to copy
# and modify as needed.

# === LAB LAYOUT ===
# When starting this container, a variety of directories for organizing the current code, data,
# tensorboard output, caches, and workspaces (optional) will get mounted (not copied) into the container.
# The default configuration below is based on this layout. You can override these locations with
# various environment variables or copy and modify this script. A detailed description of the lab layout
# can be found in:
#   - docs/overview.md
#   - docs/getting_started.md

# SUMMARY
# <lab-root>/
#   juneberry <- This is the Juneberry repo that was cloned.
#   dataroot <- This is where the source data is located, i.e. the Juneberry "dataroot".
#   tensorboard <- This is where tensorboard outputs will be stored.
#   cache <- This where the model downloads are cached.
#   <workspace> <- This is an optional external workspace directory, see docker/README.md for details.

### ENVIRONMENT VARIABLES
# JUNEBERRY_LAB             - The root of the lab instead of the CWD
# JUNEBERRY_REPOSITORY      - Path to Juneberry Repository
# JUNEBERRY_WORKSPACE       - Optional path to the workspace. (The Juneberry repo is a sample workspace.)
# JUNEBERRY_DATA_ROOT       - Path to the root of the data
# JUNEBERRY_TENSORBOARD     - Path to store the tensorboard outputs
# JUNEBERRY_CACHE           - Path to a cache to store downloads
# JUNEBERRY_CONTAINER       - The name of the container to run
# JUNEBERRY_GPUS            - The GPUS for the container: "" for none, "all" to set to all, IDs for specific ones.
# JUNEBERRY_PROFILE_NAME    - The profile name

# === Lab Layout ===
LAB=${PWD}

# If they specified a lab directory then use it as a basis for the lab structure.
if [ ! -z ${JUNEBERRY_LAB} ]; then
    LAB=${JUNEBERRY_LAB}
fi

# Standard directory layout based on the lab
REPOSITORY=${LAB}/juneberry
WORKSPACE=""
DATA_ROOT=${LAB}/dataroot
TENSORBOARD=${LAB}/tensorboard
CACHE=${LAB}/cache

# By default, the data root is mounted read only for safety.
# Under normal conditions (train/eval) Juneberry will NOT need to change the data.
DATA_ROOT_MODE=":ro"

# By default, Juneberry will use the "default" PROFILE_NAME
PROFILE_NAME="default"

# The 'cpudev' container is used by default. The 'cudadev' container is one alternative that can be used.
# If 'cudadev' is chosen, one should adjust the number of GPUs used (GPUS or JUNEBERRY_GPUS).
CONTAINER="cmusei/juneberry:cpudev"

# This value determines which GPUs will be exposed into the container. Meaning, running
# nvidia-smi inside the container will show the GPUs specified by this value.
# Set to "" for none or "all" for all.
# Specific GPUs can be set by listing the devices by ID as one would with CUDA_VISIBLE_DEVICES.
# If CUDA_VISIBLE_DEVICES is set, the script will automatically pick that up instead.
GPUS=""
# GPUS="all"
if [ ! -z ${CUDA_VISIBLE_DEVICES} ]; then
    GPUS="\"device=${CUDA_VISIBLE_DEVICES}\""
fi

# Base 'docker run' command. See "https://docs.docker.com/engine/reference/commandline/run/" for details.
# NOTE: --network=host means we can't use jupyter or expose any other ports.
# NOTE: --ipc=host is required to make GPUs work.
RUN_CMD="run -it --rm --network=host --ipc=host --name ${USER}"

# ==================================================================================================
# ==================================================================================================
# ==================================================================================================
# The remainder of this file is argument checking and command formatting

# Bring in environment variables if set
if [ ! -z ${JUNEBERRY_REPOSITORY} ]; then REPOSITORY=${JUNEBERRY_REPOSITORY}; fi
if [ ! -z ${JUNEBERRY_WORKSPACE} ]; then WORKSPACE=${JUNEBERRY_WORKSPACE}; fi
if [ ! -z ${JUNEBERRY_DATA_ROOT} ]; then DATA_ROOT=${JUNEBERRY_DATA_ROOT}; fi
if [ ! -z ${JUNEBERRY_TENSORBOARD} ]; then TENSORBOARD=${JUNEBERRY_TENSORBOARD}; fi
if [ ! -z ${JUNEBERRY_CACHE} ]; then CACHE=${JUNEBERRY_CACHE}; fi
if [ ! -z ${JUNEBERRY_CONTAINER} ]; then CONTAINER=${JUNEBERRY_CONTAINER}; fi
if [ ! -z ${JUNEBERRY_GPUS} ]; then GPUS=${JUNEBERRY_GPUS}; fi
if [ ! -z ${JUNEBERRY_PROFILE_NAME} ]; then PROFILE_NAME=${JUNEBERRY_PROFILE_NAME}; fi

# ==================================================================================================

# ===== Wrap them in the right docker format/switches
JB="-v ${REPOSITORY}:/juneberry"
if [ ! -z ${WORKSPACE} ]; then
    # Extract the workspace name so we can have that exact directory
    WS_NAME="${WORKSPACE##*/}"
    WS="-v ${WORKSPACE}:/${WS_NAME} -w /${WS_NAME}"
else
    WS=""
    JB="${JB} -w /juneberry"
fi
DR="-v ${DATA_ROOT}:/dataroot${DATA_ROOT_MODE}"
TB="-v ${TENSORBOARD}:/tensorboard"
HUB_CACHE="-v ${CACHE}/hub:/root/.cache/torch/hub"
TORCH_CACHE="-v ${CACHE}/torch:/root/.torch"
TF_CACHE="-v ${CACHE}/tensorflow:/root/tensorflow_datasets"
CACHES="${HUB_CACHE} ${TORCH_CACHE} ${TF_CACHE}"
if [ ! -z ${GPUS} ]; then
    GPUS="--gpus ${GPUS}"
fi

# ===== Check to see that the project directory contains all necessary subdirectories.
required=("${REPOSITORY}" "${WORKSPACE}" "${DATA_ROOT}" "${TENSORBOARD}" "${CACHE}")

all_lab_subdirs_exist=1
for subdir in ${required[@]}; do
    if [ ! -d ${subdir} ]; then
        echo "Lab subdirectory ${subdir} does not exist."
        all_lab_subdirs_exist=0
    fi
done

if [ ${all_lab_subdirs_exist} -eq 0 ]; then
    echo "${LAB} is not a valid lab directory."
    echo "Please run 'scripts/setup_lab.py' to create the lab directory structure."
    echo "Exiting."
    exit 1
fi

# ===== Make the cache subdirectories if missing
if [ ! -d "${CACHE}/hub" ]; then mkdir "${CACHE}/hub"; fi
if [ ! -d "${CACHE}/torch" ]; then mkdir "${CACHE}/torch"; fi
if [ ! -d "${CACHE}/tensorflow" ]; then mkdir "${CACHE}/tensorflow"; fi

# ================================================================================================

# Tunnel in a variety of common environment variables that the container can use
ENVS="--env HTTP_PROXY --env http_proxy --env HTTPS_PROXY --env https_proxy --env NO_PROXY --env no_proxy"
ENVS_USER="-e USER_NAME=${USER} -e USER_ID=$(id -u ${USER}) -e USER_GID=$(id -g ${USER}) -e HOST_UNAME=$(uname)"

# Assemble the command, show it, and run it.
CMD="docker ${RUN_CMD} ${ENVS} ${ENVS_USER} ${GPUS} ${JB} ${WS} ${DR} ${TB} ${CACHES} ${CONTAINER} bash"
echo "${CMD}"
${CMD}
